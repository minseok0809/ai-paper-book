# AI Paper
<br>

|    | Title                                                                                                         | Journal/Conference | Date       | Author                      | Link                                                          |
| -- | ------------------------------------------------------------------------------------------------------------- | ------------------ | ---------- | --------------------------- | ------------------------------------------------------------- |
| 1  | Adaptive Mixtures of Local Experts                                                                            | MIT Press 1991     | 1991-03-01 | Robert A. Jacobs et al      | [Link](https://ieeexplore.ieee.org/abstract/document/6797059) |
| 2  | BLEU: a method for automatic evaluation of machine translation                                                | ACL 2002          | 2002-07-01 | Kishore Papineni et al      | [Link](https://dl.acm.org/doi/10.3115/1073083.1073135)        |
| 3  | Model Compression                                                                                             | ACM SIGKDD 2006    | 2006-08-20 | Cristian Bucil˘a et al      | [Link](https://dl.acm.org/doi/abs/10.1145/1150402.1150464)    |
| 4  | Large-Scale Distributed Language Modeling                                                                     | IEEE 2007         | 2007-04-05 | Ahmad Emami et al           | [Link](https://ieeexplore.ieee.org/document/4218031)          |
| 5  | Large Language Models in Machine Translation                                                                  | EMNLP 2007         | 2007-06-01 | Thorsten Brants et al       | [Link](https://aclanthology.org/D07-1090/)                    |
| 6  | Efficient Estimation of Word Representations in Vector Space                                                  |                    | 2013-01-16 | Tomas Mikolov et al         | [Link](http://arxiv.org/abs/1301.3781v3)                      |
| 7  | Linguistic Regularities in Continuous Space Word Representations                                              | NAACL 2013        | 2013-06-01 | Tomas Mikolov et al         | [Link](https://aclanthology.org/N13-1090/)                    |
| 8  | Dropout: A Simple Way to Prevent Neural Networks from Overfitting                                             | JMLR 2014         | 2014-01-01 | Nitish Srivastava et al     | [Link](https://ieeexplore.ieee.org/abstract/document/6797059) |
| 9  | Neural Machine Translation by Jointly Learning to Align and Translate                                         | ICLR  2015         | 2014-09-01 | Dzmitry Bahdanau et al      | [Link](http://arxiv.org/abs/1409.0473v7)                      |
| 10 | Sequence to Sequence Learning with Neural Networks                                                            | NeurIPS 2014       | 2014-09-10 | Ilya Sutskever et al        | [Link](http://arxiv.org/abs/1409.3215v3)                      |
| 11 | Distilling the Knowledge in a Neural Network                                                                  | NIPS  2014         | 2015-03-09 | Geoffrey Hinton et al       | [Link](http://arxiv.org/abs/1503.02531v1)                     |
| 12 | Neural Machine Translation of Rare Words with Subword Units                                                   | ACL  2016          | 2015-08-31 | Rico Sennrich et al         | [Link](http://arxiv.org/abs/1508.07909v5)                     |
| 13 | Adaptive Computation Time for Recurrent Neural Networks                                                       |                    | 2016-03-29 | Alex Graves et al           | [Link](http://arxiv.org/abs/1603.08983v6)                     |
| 14 | Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation            |                    | 2016-09-26 | Yonghui Wu et al            | [Link](http://arxiv.org/abs/1609.08144v2)                     |
| 15 | Attention Is All You Need                                                                                     | NeurIPS 2017       | 2017-06-12 | Ashish Vaswani et al        | [Link](http://arxiv.org/abs/1706.03762v5)                     |
| 16 | BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks                                        | IEEE 2016          | 2017-09-06 | Surat Teerapittayanon et al | [Link](http://arxiv.org/abs/1709.01686v1)                     |
| 17 | Deep contextualized word representations                                                                      | NAACL  2018        | 2018-02-15 | Matthew E. Peters et al     | [Link](http://arxiv.org/abs/1802.05365v2)                     |
| 18 | GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding                         | ICLR  2019         | 2018-04-20 | Alex Wang et al             | [Link](http://arxiv.org/abs/1804.07461v3)                     |
| 19 | SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing | EMNLP 2018         | 2018-08-19 | Taku Kudo et al             | [Link](http://arxiv.org/abs/1808.06226v1)                     |
| 20 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding                              | ACL  2019          | 2018-10-11 | Jacob Devlin et al          | [Link](http://arxiv.org/abs/1810.04805v2)                     |
| 21 | Improved Knowledge Distillation via Teacher Assistant                                                         | AAAI  2020         | 2019-02-09 | Seyed-Iman Mirzadeh et al   | [Link](http://arxiv.org/abs/1902.03393v2)                     |
| 22 | Patient Knowledge Distillation for BERT Model Compression                                                     | EMNLP  2019        | 2019-08-25 | Siqi Sun et al              | [Link](http://arxiv.org/abs/1908.09355v1)                     |
| 23 | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter                                 | NeurIPS 2019       | 2019-10-02 | Victor Sanh et al           | [Link](http://arxiv.org/abs/1910.01108v4)                     |
| 24 | MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers            | NeurIPS 2020       | 2020-02-25 | Wenhui Wang et al           | [Link](http://arxiv.org/abs/2002.10957v2)                     |
| 25 | FastBERT: a Self-distilling BERT with Adaptive Inference Time                                                 | ACL  2020          | 2020-04-05 | Weijie Liu et al            | [Link](http://arxiv.org/abs/2004.02178v2)                     |
| 26 | Reliable Post hoc Explanations: Modeling Uncertainty in Explainability                                        | NeurIPS 2021       | 2020-08-11 | Dylan Slack et al           | [Link](http://arxiv.org/abs/2008.05030v4)                     |
| 27 | MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers             |                    | 2020-12-31 | Wenhui Wang et al           | [Link](http://arxiv.org/abs/2012.15828v2)                     |
| 28 | Ensemble deep learning: A review                                                                              | AAAI  2022         | 2021-04-06 | M. A. Ganaie et al          | [Link](http://arxiv.org/abs/2104.02395v3)                     |
| 29 | A Survey on Model Compression and Acceleration for Pretrained Language Models                                 | AAAI  2023         | 2022-02-15 | Canwen Xu et al             | [Link](http://arxiv.org/abs/2202.07105v2)                     |          |

<br><br><br><br>

# AI Book
<br>

|   | Title                                                                                                 | Author                   | Publisher               | Year |
| - | ----------------------------------------------------------------------------------------------------- | ------------------------ | ----------------------- | ---- |
| 1 | Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning | Delip Rao, Brian McMahan | O'Reilly Media         | 2019 |
| 2 | Interpretable Machine Learning: A Guide for Making Black Box Models Explainable                       | Christoph Molnar         | Independently published | 2022 |

<br><br><br><br>

# Future Work
<br><b>Google Schloar Crawler</b>
<br>What's recent paper cited by the paper

<br><br><br><br>

