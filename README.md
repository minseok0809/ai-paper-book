# AI Paper
<br>

| Theme                                     | Number                                                                                                            | Title                                                                                      | Journal/Conference                        | Author                                                                                                                                                | Link                                                          |
| :-----------------------------------------: | :-----------------------------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------ | ----------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| Neural Network                            | 1                                                                                                                 | Dropout: A Simple Way to Prevent Neural Networks from Overfitting                          | JMLR 2014                                 | Nitish Srivastava et al                                                                                                                               | [Link](https://ieeexplore.ieee.org/abstract/document/6797059) |
| 2                                         | Adaptive Computation Time for Recurrent Neural Networks                                                           |                                                                                            | Alex Graves et al                         | [Link](http://arxiv.org/abs/1603.08983v6)                                                                                                             |
| 3                                         | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding                                  | ACL 2019                                                                                   | Jacob Devlin et al                        | [Link](http://arxiv.org/abs/1810.04805v2)                                                                                                             |
| Benchmark                                 | 4                                                                                                                 | BLEU: a method for automatic evaluation of machine translation                             | ACL 2002                                  | Kishore Papineni et al                                                                                                                                | [Link](https://dl.acm.org/doi/10.3115/1073083.1073135)        |
| 5                                         | GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding                             | ICLR  2019                                                                                 | Alex Wang et al                           | [Link](http://arxiv.org/abs/1804.07461v3)                                                                                                             |
| Word Embedding                            | 6                                                                                                                 | Large-Scale Distributed Language Modeling                                                  | IEEE 2007                                 | Ahmad Emami et al                                                                                                                                     | [Link](https://ieeexplore.ieee.org/document/4218031)          |
| 7                                         | Efficient Estimation of Word Representations in Vector Space                                                      |                                                                                            | Tomas Mikolov et al                       | [Link](http://arxiv.org/abs/1301.3781v3)                                                                                                              |
| 8                                         | Linguistic Regularities in Continuous Space Word Representations                                                  | NAACL 2013                                                                                 | Tomas Mikolov et al                       | [Link](https://aclanthology.org/N13-1090/)                                                                                                            |
| 9                                         | Deep contextualized word representations                                                                          | NAACL  2018                                                                                | Matthew E. Peters et al                   | [Link](http://arxiv.org/abs/1802.05365v2)                                                                                                             |
| 10                                        | SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing     | EMNLP 2018                                                                                 | Taku Kudo et al                           | [Link](http://arxiv.org/abs/1808.06226v1)                                                                                                             |
| Explainable Artificial Intelligence (XAI) | 11                                                                                                                | Student-Centered Learning in Higher Education                                              | Teaching and Learning in Higher Education | Thorsten Brants et al                                                                                                                                 | [Link](https://files.eric.ed.gov/fulltext/EJ938583.pdf)       |
| 12                                        | Building Machines That Learn and Think Like People                                                                | Behavioral and Brain Sciences                                                              | Brenden M. Lake et al                     | [Link](http://arxiv.org/abs/1604.00289v3)                                                                                                             |
| 13                                        | A Multiscale Visualization of Attention in the Transformer Model                                                  | ACL  2019                                                                                  | Jesse Vig et al                           | [Link](http://arxiv.org/abs/1906.05714v1)                                                                                                             |
| 14                                        | Reliable Post hoc Explanations: Modeling Uncertainty in Explainability                                            | NeurIPS 2021                                                                               | Dylan Slack et al                         | [Link](http://arxiv.org/abs/2008.05030v4)                                                                                                             |
| Pre-Trained Language Model                | 15                                                                                                                | Unified Language Model Pre-training for Natural Language Understanding and Generation      | NeurIPS 2019                              | Li Dong et al                                                                                                                                         | [Link](http://arxiv.org/abs/1905.03197v3)                     |
| 16                                        | Language Models are Unsupervised Multitask Learners                                                               | OpenAI                                                                                     | Alec Radford et al                        | [Link](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe) |
| 17                                        | SpanBERT: Improving Pre-training by Representing and Predicting Spans                                             | TACL 2020                                                                                  | Mandar Joshi et al                        | [Link](http://arxiv.org/abs/1907.10529v3)                                                                                                             |
| 18                                        | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | ACL  2020                                                                                  | Mike Lewis et al                          | [Link](http://arxiv.org/abs/1910.13461v1)                                                                                                             |
| 19                                        | A Survey on Model Compression and Acceleration for Pretrained Language Models                                     | AAAI  2023                                                                                 | Canwen Xu et al                           | [Link](http://arxiv.org/abs/2202.07105v2)                                                                                                             |
| Multimodal Learning                       | 20                                                                                                                | Show, Attend and Tell: Neural Image Caption Generation with Visual Attention               | ICML 2015                                 | Kelvin Xu et al                                                                                                                                       | [Link](http://arxiv.org/abs/1502.03044v3)                     |
| Image Classification                      | 21                                                                                                                | XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks               | ECCV 2016                                 | Mohammad Rastegari et al                                                                                                                              | [Link](http://arxiv.org/abs/1603.05279v4)                     |
| 22                                        | Do CIFAR-10 Classifiers Generalize to CIFAR-10?                                                                   |                                                                                            | Benjamin Recht et al                      | [Link](http://arxiv.org/abs/1806.00451v1)                                                                                                             |
| Machine Translation                       | 23                                                                                                                | Large Language Models in Machine Translation                                               | EMNLP 2007                                | Gloria Brown Wright et al                                                                                                                             | [Link](https://aclanthology.org/D07-1090/)                    |
| 24                                        | Neural Machine Translation by Jointly Learning to Align and Translate                                             | ICLR  2015                                                                                 | Dzmitry Bahdanau et al                    | [Link](http://arxiv.org/abs/1409.0473v7)                                                                                                              |
| 25                                        | Sequence to Sequence Learning with Neural Networks                                                                | NeurIPS 2014                                                                               | Ilya Sutskever et al                      | [Link](http://arxiv.org/abs/1409.3215v3)                                                                                                              |
| 26                                        | Neural Machine Translation of Rare Words with Subword Units                                                       | ACL  2016                                                                                  | Rico Sennrich et al                       | [Link](http://arxiv.org/abs/1508.07909v5)                                                                                                             |
| 27                                        | Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation                |                                                                                            | Yonghui Wu et al                          | [Link](http://arxiv.org/abs/1609.08144v2)                                                                                                             |
| 28                                        | Attention Is All You Need                                                                                         | NeurIPS 2017                                                                               | Ashish Vaswani et al                      | [Link](http://arxiv.org/abs/1706.03762v5)                                                                                                             |
| Natural Language Understanding (NLU)      | 29                                                                                                                | Adversarial Examples for Evaluating Reading Comprehension Systems                          | EMNLP  2017                               | Robin Jia et al                                                                                                                                       | [Link](http://arxiv.org/abs/1707.07328v1)                     |
| Meta Learning                             | 30                                                                                                                | Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks                          | ICML  2017                                | Chelsea Finn et al                                                                                                                                    | [Link](http://arxiv.org/abs/1703.03400v3)                     |
| 31                                        | Optimization as A Model for Few-shot Learning                                                                     | ICLR 2017                                                                                  | Sachin Ravi et al                         | [Link](https://openreview.net/forum?id=rJY0-Kcll)                                                                                                     |
| 32                                        | BERT Learns to Teach: Knowledge Distillation with Meta Learning                                                   | ACL  2022                                                                                  | Wangchunshu Zhou et al                    | [Link](http://arxiv.org/abs/2106.04570v3)                                                                                                             |
| Continual Learning                        | 33                                                                                                                | Overcoming catastrophic forgetting in neural networks                                      |                                           | James Kirkpatrick et al                                                                                                                               | [Link](http://arxiv.org/abs/1612.00796v2)                     |
| Mixture of Experts                        | 34                                                                                                                | Adaptive Mixtures of Local Experts                                                         | MIT Press 1991                            | Robert A. Jacobs et al                                                                                                                                | [Link](https://ieeexplore.ieee.org/abstract/document/6797059) |
| Ensemble                                  | 35                                                                                                                | Ensemble deep learning: A review                                                           | AAAI 2022                                 | M. A. Ganaie et al                                                                                                                                    | [Link](http://arxiv.org/abs/2104.02395v3)                     |
| Model Compression                         | 36                                                                                                                | Model Compression                                                                          | ACM SIGKDD 2006                           | Cristian Bucil˘a et al                                                                                                                                | [Link](https://dl.acm.org/doi/abs/10.1145/1150402.1150464)    |
| Knoweldge Distillation                    | 37                                                                                                                | Distilling the Knowledge in a Neural Network                                               | NIPS  2014                                | Geoffrey Hinton et al                                                                                                                                 | [Link](http://arxiv.org/abs/1503.02531v1)                     |
| 38                                        | Improved Knowledge Distillation via Teacher Assistant                                                             | AAAI  2020                                                                                 | Seyed-Iman Mirzadeh et al                 | [Link](http://arxiv.org/abs/1902.03393v2)                                                                                                             |
| 39                                        | Patient Knowledge Distillation for BERT Model Compression                                                         | EMNLP  2019                                                                                | Siqi Sun et al                            | [Link](http://arxiv.org/abs/1908.09355v1)                                                                                                             |
| 40                                        | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter                                     | NeurIPS 2019                                                                               | Victor Sanh et al                         | [Link](http://arxiv.org/abs/1910.01108v4)                                                                                                             |
| 41                                        | MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers                | NeurIPS 2020                                                                               | Wenhui Wang et al                         | [Link](http://arxiv.org/abs/2002.10957v2)                                                                                                             |
| 42                                        | FastBERT: a Self-distilling BERT with Adaptive Inference Time                                                     | ACL  2020                                                                                  | Weijie Liu et al                          | [Link](http://arxiv.org/abs/2004.02178v2)                                                                                                             |
| 43                                        | MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers                 | ACL 2021 (Findings)                                                                        | Wenhui Wang et al                         | [Link](http://arxiv.org/abs/2012.15828v2)                                                                                                             |
| Quantization                              | 44                                                                                                                | Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation |                                           | Yoshua Bengio et al                                                                                                                                   | [Link](http://arxiv.org/abs/1308.3432v1)                      |
| 45                                        | Convolutional Neural Networks using Logarithmic Data Representation                                               | ECCV 2016                                                                                  | Daisuke Miyashita et al                   | [Link](http://arxiv.org/abs/1603.01025v2)                                                                                                             |
| 46                                        | Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations                    |                                                                                            | Itay Hubara et al                         | [Link](http://arxiv.org/abs/1609.07061v1)                                                                                                             |
| 47                                        | Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference                      | CVPR 2018                                                                                  | Benoit Jacob et al                        | [Link](http://arxiv.org/abs/1712.05877v1)                                                                                                             |
| 48                                        | HAQ: Hardware-Aware Automated Quantization with Mixed Precision                                                   | CVPR  2019                                                                                 | Kuan Wang et al                           | [Link](http://arxiv.org/abs/1811.08886v3)                                                                                                             |
| 49                                        | And the Bit Goes Down: Revisiting the Quantization of Neural Networks                                             | ICLR  2020                                                                                 | Pierre Stock et al                        | [Link](http://arxiv.org/abs/1907.05686v5)                                                                                                             |
| 50                                        | Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT                                                    | AAAI 2020                                                                                  | Sheng Shen et al                          | [Link](http://arxiv.org/abs/1909.05840v2)                                                                                                             |
| 51                                        | Quantization Networks                                                                                             | CVPR 2019                                                                                  | Jiwei Yang et al                          | [Link](http://arxiv.org/abs/1911.09464v2)                                                                                                             |
| 52                                        | Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation                             |                                                                                            | Hao Wu et al                              | [Link](http://arxiv.org/abs/2004.09602v1)                                                                                                             |
| 53                                        | A Survey of Quantization Methods for Efficient Neural Network Inference                                           |                                                                                            | Amir Gholami et al                        | [Link](http://arxiv.org/abs/2103.13630v3)                                                                                                             |
| 54                                        | BiBERT: Accurate Fully Binarized BERT                                                                             | ICLR 2022                                                                                  | Haotong Qin et al                         | [Link](http://arxiv.org/abs/2203.06390v1)                                                                                                             |
| Early Exit                                | 55                                                                                                                | BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks                     |                                           | Surat Teerapittayanon et al                                                                                                                           | [Link](http://arxiv.org/abs/1709.01686v1)                     |

<br><br><br/><br/>

# Theme
`Neural Network` `Benchmark` `Word Embedding`
<br/>`Explainable Artificial Intelligence (XAI)`
<br/>`Language Model` `Pre-Trained Language Model` 
<br/>`Multimodal Learning` `Image Classification` 
<br/>`Machine Translation` `Natural Language Understanding (NLU)` `Text Generation`
<br/>`Meta Learning` `Continual Learning` `Mixture of Experts` `Ensemble` 
<br/>`Model Compression` `Knoweldge Distillation` `Quantization` `Pruning` 
<br/>`Low-Rank Factorization` `Early Exit`

<br/><br/><br/><br/><br/>


# Future Work
<br><b>Google Schloar Crawler</b>
<br>What's recent paper cited by the paper

<br><br><br><br>

