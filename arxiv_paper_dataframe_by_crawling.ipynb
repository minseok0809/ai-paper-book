{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Paper Dataframe by Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract = pd.DataFrame({'Title':[0],\n",
    "                               'Journal/Conference':[0],\n",
    "                               'Date':[0], \n",
    "                               'Author':[0],\n",
    "                               'Link':[0],\n",
    "                               'Abstract':[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = [\"1409.0473v7\", \"1409.3215v3\", \"1706.03762v5\", \"1609.08144v2\",\n",
    "             \"1508.07909v5\", \"1301.3781v3\", \"1808.06226v1\", \"1802.05365v2\",\n",
    "             \"1810.04805v2\", \"2104.02395v3\", \"2202.07105v2\", \"1503.02531v1\",\n",
    "             \"1910.01108v4\", \"1908.09355v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, paper_id in enumerate(paper_ids):\n",
    "    search = arxiv.Search(id_list=[paper_id])\n",
    "    paper = next(search.results())\n",
    "    \n",
    "    paper_journal_conf = re.search(r'[A-Z ]+[0-9]+[0-9]+[0-9]+[0-9]', str(paper.comment))\n",
    "    if paper_journal_conf != None:\n",
    "        paper_journal_conf = paper_journal_conf.group().strip()\n",
    "        if len(paper_journal_conf) > 4:\n",
    "            if paper_journal_conf[-4] != \" \":\n",
    "                paper_journal_conf = paper_journal_conf[:-4] + \" \" + paper_journal_conf[-4:]\n",
    "            else:\n",
    "                paper_journal_conf = paper_journal_conf\n",
    "        elif len(paper_journal_conf) <= 4:\n",
    "            paper_journal_conf = \"\"\n",
    "    elif paper_journal_conf == None:\n",
    "        paper_journal_conf = \"\"\n",
    "\n",
    "    arxiv_paper_df_with_abstract.loc[idx] = [paper.title, \n",
    "                              paper_journal_conf,\n",
    "                               paper.published.date(), \n",
    "                               str(paper.authors[0]) + ' et al.',\n",
    "                                 paper.entry_id,\n",
    "                                 paper.summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_convert_datetime(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_papers = [[\"Model Compression\", \"ACM SIGKDD 2006\", str_convert_datetime(\"2006-08-20\"),\n",
    "                 \"Cristian Bucil˘a et al.\", \"https://dl.acm.org/doi/abs/10.1145/1150402.1150464\", \n",
    "                 \"Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for 'compressing' large, complex ensembles into smaller, faster models, usually without significant loss in performance.\"],\n",
    "                 [\"Adaptive Mixtures of Local Experts\", \"MIT Press 1991\", str_convert_datetime(\"1991-03-01\"),\n",
    "                 \"Robert A. Jacobs et al.\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                 \"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.\"],\n",
    "                 [\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", \" JMLR 2014\", str_convert_datetime(\"2014-01-01\"),\n",
    "                 \"Nitish Srivastava et al.\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                 \"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\"]\n",
    "                 ]\n",
    "df_length = len(arxiv_paper_df_with_abstract) - 1\n",
    "\n",
    "for other_paper in other_papers:\n",
    "  df_length += 1\n",
    "  arxiv_paper_df_with_abstract.loc[df_length-1] = other_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)    \n",
    "arxiv_paper_df = arxiv_paper_df_with_abstract.drop(['Abstract'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaptive Mixtures of Local Experts</td>\n",
       "      <td>MIT Press 1991</td>\n",
       "      <td>1991-03-01</td>\n",
       "      <td>Robert A. Jacobs et al.</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "      <td>We present a new supervised learning procedure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model Compression</td>\n",
       "      <td>ACM SIGKDD 2006</td>\n",
       "      <td>2006-08-20</td>\n",
       "      <td>Cristian Bucil˘a et al.</td>\n",
       "      <td>https://dl.acm.org/doi/abs/10.1145/1150402.115...</td>\n",
       "      <td>Often the best performing supervised learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Efficient Estimation of Word Representations i...</td>\n",
       "      <td></td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>Tomas Mikolov et al.</td>\n",
       "      <td>http://arxiv.org/abs/1301.3781v3</td>\n",
       "      <td>We propose two novel model architectures for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dropout: A Simple Way to Prevent Neural Networ...</td>\n",
       "      <td>JMLR 2014</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>Nitish Srivastava et al.</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "      <td>Deep neural nets with a large number of parame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Neural Machine Translation by Jointly Learning...</td>\n",
       "      <td>ICLR  2015</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>Dzmitry Bahdanau et al.</td>\n",
       "      <td>http://arxiv.org/abs/1409.0473v7</td>\n",
       "      <td>Neural machine translation is a recently propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td></td>\n",
       "      <td>2014-09-10</td>\n",
       "      <td>Ilya Sutskever et al.</td>\n",
       "      <td>http://arxiv.org/abs/1409.3215v3</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Distilling the Knowledge in a Neural Network</td>\n",
       "      <td>NIPS  2014</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>Geoffrey Hinton et al.</td>\n",
       "      <td>http://arxiv.org/abs/1503.02531v1</td>\n",
       "      <td>A very simple way to improve the performance o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neural Machine Translation of Rare Words with ...</td>\n",
       "      <td>ACL  2016</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Rico Sennrich et al.</td>\n",
       "      <td>http://arxiv.org/abs/1508.07909v5</td>\n",
       "      <td>Neural machine translation (NMT) models typica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Google's Neural Machine Translation System: Br...</td>\n",
       "      <td></td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>Yonghui Wu et al.</td>\n",
       "      <td>http://arxiv.org/abs/1609.08144v2</td>\n",
       "      <td>Neural Machine Translation (NMT) is an end-to-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td></td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>Ashish Vaswani et al.</td>\n",
       "      <td>http://arxiv.org/abs/1706.03762v5</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>NAACL  2018</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>Matthew E. Peters et al.</td>\n",
       "      <td>http://arxiv.org/abs/1802.05365v2</td>\n",
       "      <td>We introduce a new type of deep contextualized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SentencePiece: A simple and language independe...</td>\n",
       "      <td>EMNLP 2018</td>\n",
       "      <td>2018-08-19</td>\n",
       "      <td>Taku Kudo et al.</td>\n",
       "      <td>http://arxiv.org/abs/1808.06226v1</td>\n",
       "      <td>This paper describes SentencePiece, a language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>Jacob Devlin et al.</td>\n",
       "      <td>http://arxiv.org/abs/1810.04805v2</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>Victor Sanh et al.</td>\n",
       "      <td>http://arxiv.org/abs/1910.01108v4</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ensemble deep learning: A review</td>\n",
       "      <td></td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>M. A. Ganaie et al.</td>\n",
       "      <td>http://arxiv.org/abs/2104.02395v3</td>\n",
       "      <td>Ensemble learning combines several individual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A Survey on Model Compression and Acceleration...</td>\n",
       "      <td>AAAI  2023</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>Canwen Xu et al.</td>\n",
       "      <td>http://arxiv.org/abs/2202.07105v2</td>\n",
       "      <td>Despite achieving state-of-the-art performance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title Journal/Conference   \n",
       "1                  Adaptive Mixtures of Local Experts     MIT Press 1991  \\\n",
       "2                                   Model Compression    ACM SIGKDD 2006   \n",
       "3   Efficient Estimation of Word Representations i...                      \n",
       "4   Dropout: A Simple Way to Prevent Neural Networ...          JMLR 2014   \n",
       "5   Neural Machine Translation by Jointly Learning...         ICLR  2015   \n",
       "6   Sequence to Sequence Learning with Neural Netw...                      \n",
       "7        Distilling the Knowledge in a Neural Network         NIPS  2014   \n",
       "8   Neural Machine Translation of Rare Words with ...          ACL  2016   \n",
       "9   Google's Neural Machine Translation System: Br...                      \n",
       "10                          Attention Is All You Need                      \n",
       "11           Deep contextualized word representations        NAACL  2018   \n",
       "12  SentencePiece: A simple and language independe...         EMNLP 2018   \n",
       "13  BERT: Pre-training of Deep Bidirectional Trans...                      \n",
       "14  DistilBERT, a distilled version of BERT: small...                      \n",
       "15                   Ensemble deep learning: A review                      \n",
       "16  A Survey on Model Compression and Acceleration...         AAAI  2023   \n",
       "\n",
       "          Date                    Author   \n",
       "1   1991-03-01   Robert A. Jacobs et al.  \\\n",
       "2   2006-08-20   Cristian Bucil˘a et al.   \n",
       "3   2013-01-16      Tomas Mikolov et al.   \n",
       "4   2014-01-01  Nitish Srivastava et al.   \n",
       "5   2014-09-01   Dzmitry Bahdanau et al.   \n",
       "6   2014-09-10     Ilya Sutskever et al.   \n",
       "7   2015-03-09    Geoffrey Hinton et al.   \n",
       "8   2015-08-31      Rico Sennrich et al.   \n",
       "9   2016-09-26         Yonghui Wu et al.   \n",
       "10  2017-06-12     Ashish Vaswani et al.   \n",
       "11  2018-02-15  Matthew E. Peters et al.   \n",
       "12  2018-08-19          Taku Kudo et al.   \n",
       "13  2018-10-11       Jacob Devlin et al.   \n",
       "14  2019-10-02        Victor Sanh et al.   \n",
       "15  2021-04-06       M. A. Ganaie et al.   \n",
       "16  2022-02-15          Canwen Xu et al.   \n",
       "\n",
       "                                                 Link   \n",
       "1   https://ieeexplore.ieee.org/abstract/document/...  \\\n",
       "2   https://dl.acm.org/doi/abs/10.1145/1150402.115...   \n",
       "3                    http://arxiv.org/abs/1301.3781v3   \n",
       "4   https://ieeexplore.ieee.org/abstract/document/...   \n",
       "5                    http://arxiv.org/abs/1409.0473v7   \n",
       "6                    http://arxiv.org/abs/1409.3215v3   \n",
       "7                   http://arxiv.org/abs/1503.02531v1   \n",
       "8                   http://arxiv.org/abs/1508.07909v5   \n",
       "9                   http://arxiv.org/abs/1609.08144v2   \n",
       "10                  http://arxiv.org/abs/1706.03762v5   \n",
       "11                  http://arxiv.org/abs/1802.05365v2   \n",
       "12                  http://arxiv.org/abs/1808.06226v1   \n",
       "13                  http://arxiv.org/abs/1810.04805v2   \n",
       "14                  http://arxiv.org/abs/1910.01108v4   \n",
       "15                  http://arxiv.org/abs/2104.02395v3   \n",
       "16                  http://arxiv.org/abs/2202.07105v2   \n",
       "\n",
       "                                             Abstract  \n",
       "1   We present a new supervised learning procedure...  \n",
       "2   Often the best performing supervised learning ...  \n",
       "3   We propose two novel model architectures for c...  \n",
       "4   Deep neural nets with a large number of parame...  \n",
       "5   Neural machine translation is a recently propo...  \n",
       "6   Deep Neural Networks (DNNs) are powerful model...  \n",
       "7   A very simple way to improve the performance o...  \n",
       "8   Neural machine translation (NMT) models typica...  \n",
       "9   Neural Machine Translation (NMT) is an end-to-...  \n",
       "10  The dominant sequence transduction models are ...  \n",
       "11  We introduce a new type of deep contextualized...  \n",
       "12  This paper describes SentencePiece, a language...  \n",
       "13  We introduce a new language representation mod...  \n",
       "14  As Transfer Learning from large-scale pre-trai...  \n",
       "15  Ensemble learning combines several individual ...  \n",
       "16  Despite achieving state-of-the-art performance...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract.to_excel(\"arxiv_paper_df_with_abstract.xlsx\")\n",
    "arxiv_paper_df.to_excel(\"arxiv_paper_df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataframe on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Excel to Markdown Converter](https://tabletomarkdown.com/convert-spreadsheet-to-markdown/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
