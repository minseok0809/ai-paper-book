{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Paper Dataframe by Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_with_abstract(paper_ids):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame({'Title':[0],\n",
    "                               'Journal/Conference':[0],\n",
    "                               'Date':[0], \n",
    "                               'Author':[0],\n",
    "                               'Link':[0],\n",
    "                               'Abstract':[0]})\n",
    "    \n",
    "    for idx, paper_id in enumerate(paper_ids):\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(search.results())\n",
    "        \n",
    "        paper_journal_conf = re.search(r'[A-Z ]+[0-9]+[0-9]+[0-9]+[0-9]', str(paper.comment))\n",
    "        if paper_journal_conf != None:\n",
    "            paper_journal_conf = paper_journal_conf.group().strip()\n",
    "            if len(paper_journal_conf) > 4:\n",
    "                if paper_journal_conf[-4] != \" \":\n",
    "                    paper_journal_conf = paper_journal_conf[:-4] + \" \" + paper_journal_conf[-4:]\n",
    "                else:\n",
    "                    paper_journal_conf = paper_journal_conf\n",
    "            elif len(paper_journal_conf) <= 4:\n",
    "                paper_journal_conf = \"\"\n",
    "        elif paper_journal_conf == None:\n",
    "            paper_journal_conf = \"\"\n",
    "\n",
    "        arxiv_paper_df_with_abstract.loc[idx] = [paper.title, \n",
    "                                paper_journal_conf,\n",
    "                                paper.published.date(), \n",
    "                                str(paper.authors[0]) + ' et al',\n",
    "                                    paper.entry_id,\n",
    "                                    paper.summary]\n",
    "\n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_convert_datetime(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_other_papers_column(arxiv_paper_df_with_abstract, other_papers):\n",
    "  \n",
    "  df_length = len(arxiv_paper_df_with_abstract) - 1\n",
    "\n",
    "  for other_paper in other_papers:\n",
    "    df_length += 1\n",
    "    arxiv_paper_df_with_abstract.loc[df_length] = other_paper\n",
    "  \n",
    "  return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperlink(x):\n",
    "    hyperlink= '[Link]' + '(' + x + ')'\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)    \n",
    "    arxiv_paper_df = arxiv_paper_df_with_abstract.drop(['Abstract'], axis='columns')\n",
    "\n",
    "    return arxiv_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = [\"1409.0473v7\", \"1409.3215v3\", \"1706.03762v5\", \"1609.08144v2\",\n",
    "             \"1508.07909v5\", \"1301.3781v3\", \"1808.06226v1\", \"1802.05365v2\",\n",
    "             \"1810.04805v2\", \"2104.02395v3\", \"2202.07105v2\", \"1503.02531v1\",\n",
    "             \"1910.01108v4\", \"1908.09355v1\", \"2008.05030v4\", \"1603.08983v6\",\n",
    "             \"1709.01686v1\", \"1804.07461v3\", \"1902.03393v2\", \"2004.02178v2\",\n",
    "             \"2002.10957v2\", \"2012.15828v2\"]\n",
    "\n",
    "arxiv_paper_df_with_abstract = make_arxiv_paper_df_with_abstract(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_papers = [[\"Model Compression\", \"ACM SIGKDD 2006\", str_convert_datetime(\"2006-08-20\"),\n",
    "                \"Cristian Bucil˘a et al\", \"https://dl.acm.org/doi/abs/10.1145/1150402.1150464\", \n",
    "                \"Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for 'compressing' large, complex ensembles into smaller, faster models, usually without significant loss in performance.\"],\n",
    "                [\"Adaptive Mixtures of Local Experts\", \"MIT Press 1991\", str_convert_datetime(\"1991-03-01\"),\n",
    "                \"Robert A. Jacobs et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.\"],\n",
    "                [\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", \" JMLR 2014\", str_convert_datetime(\"2014-01-01\"),\n",
    "                \"Nitish Srivastava et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\"],\n",
    "                [\"Linguistic Regularities in Continuous Space Word Representations\", \" NAACL 2013\", str_convert_datetime(\"2013-06-01\"),\n",
    "                \"Tomas Mikolov et al\", \"https://aclanthology.org/N13-1090/\", \n",
    "                \"Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.\"],\n",
    "                 [\"Large-Scale Distributed Language Modeling\", \" IEEE 2007\", str_convert_datetime(\"2007-04-05\"),\n",
    "                \"Ahmad Emami et al\", \"https://ieeexplore.ieee.org/document/4218031\", \n",
    "                \"A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.\"],\n",
    "                 [\"BLEU: a method for automatic evaluation of machine translation\", \" ACL 2002\", str_convert_datetime(\"2002-07-01\"),\n",
    "                \"Kishore Papineni et al\", \"https://dl.acm.org/doi/10.3115/1073083.1073135\", \n",
    "                \"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.\"],\n",
    "                [\"Large Language Models in Machine Translation\", \" EMNLP 2007\", str_convert_datetime(\"2007-06-01\"),\n",
    "                \"Thorsten Brants et al\", \"https://aclanthology.org/D07-1090/\", \n",
    "                \"This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train onup to 2 trillion tokens, resulting in languagemodels having up to 300 billion n-grams. Itis capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbedStupid Backoff, that is inexpensive to trainon large data sets and approaches the qualityof Kneser-Ney Smoothing as the amount oftraining data increases.\"]\n",
    "                ]\n",
    "\n",
    "arxiv_paper_df_with_abstract = add_other_papers_column(arxiv_paper_df_with_abstract, other_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract[\"Link\"] = arxiv_paper_df_with_abstract[\"Link\"].apply(hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df = make_arxiv_paper_df(arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Machine Translation by Jointly Learning...</td>\n",
       "      <td>ICLR  2015</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>Dzmitry Bahdanau et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1409.0473v7)</td>\n",
       "      <td>Neural machine translation is a recently propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td></td>\n",
       "      <td>2014-09-10</td>\n",
       "      <td>Ilya Sutskever et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1409.3215v3)</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td></td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>Ashish Vaswani et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1706.03762v5)</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google's Neural Machine Translation System: Br...</td>\n",
       "      <td></td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>Yonghui Wu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1609.08144v2)</td>\n",
       "      <td>Neural Machine Translation (NMT) is an end-to-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Machine Translation of Rare Words with ...</td>\n",
       "      <td>ACL  2016</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Rico Sennrich et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1508.07909v5)</td>\n",
       "      <td>Neural machine translation (NMT) models typica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Efficient Estimation of Word Representations i...</td>\n",
       "      <td></td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>Tomas Mikolov et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1301.3781v3)</td>\n",
       "      <td>We propose two novel model architectures for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SentencePiece: A simple and language independe...</td>\n",
       "      <td>EMNLP 2018</td>\n",
       "      <td>2018-08-19</td>\n",
       "      <td>Taku Kudo et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1808.06226v1)</td>\n",
       "      <td>This paper describes SentencePiece, a language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>NAACL  2018</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>Matthew E. Peters et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1802.05365v2)</td>\n",
       "      <td>We introduce a new type of deep contextualized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>Jacob Devlin et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1810.04805v2)</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ensemble deep learning: A review</td>\n",
       "      <td></td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>M. A. Ganaie et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2104.02395v3)</td>\n",
       "      <td>Ensemble learning combines several individual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A Survey on Model Compression and Acceleration...</td>\n",
       "      <td>AAAI  2023</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>Canwen Xu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2202.07105v2)</td>\n",
       "      <td>Despite achieving state-of-the-art performance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Distilling the Knowledge in a Neural Network</td>\n",
       "      <td>NIPS  2014</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>Geoffrey Hinton et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1503.02531v1)</td>\n",
       "      <td>A very simple way to improve the performance o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>Victor Sanh et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1910.01108v4)</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Patient Knowledge Distillation for BERT Model ...</td>\n",
       "      <td>EMNLP  2019</td>\n",
       "      <td>2019-08-25</td>\n",
       "      <td>Siqi Sun et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1908.09355v1)</td>\n",
       "      <td>Pre-trained language models such as BERT have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Reliable Post hoc Explanations: Modeling Uncer...</td>\n",
       "      <td></td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>Dylan Slack et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2008.05030v4)</td>\n",
       "      <td>As black box explanations are increasingly bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Adaptive Computation Time for Recurrent Neural...</td>\n",
       "      <td></td>\n",
       "      <td>2016-03-29</td>\n",
       "      <td>Alex Graves et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1603.08983v6)</td>\n",
       "      <td>This paper introduces Adaptive Computation Tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BranchyNet: Fast Inference via Early Exiting f...</td>\n",
       "      <td></td>\n",
       "      <td>2017-09-06</td>\n",
       "      <td>Surat Teerapittayanon et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1709.01686v1)</td>\n",
       "      <td>Deep neural networks are state of the art meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GLUE: A Multi-Task Benchmark and Analysis Plat...</td>\n",
       "      <td>ICLR  2019</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>Alex Wang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1804.07461v3)</td>\n",
       "      <td>For natural language understanding (NLU) techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Improved Knowledge Distillation via Teacher As...</td>\n",
       "      <td>AAAI  2020</td>\n",
       "      <td>2019-02-09</td>\n",
       "      <td>Seyed-Iman Mirzadeh et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1902.03393v2)</td>\n",
       "      <td>Despite the fact that deep neural networks are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FastBERT: a Self-distilling BERT with Adaptive...</td>\n",
       "      <td>ACL  2020</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>Weijie Liu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2004.02178v2)</td>\n",
       "      <td>Pre-trained language models like BERT have pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MiniLM: Deep Self-Attention Distillation for T...</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>Wenhui Wang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2002.10957v2)</td>\n",
       "      <td>Pre-trained language models (e.g., BERT (Devli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MiniLMv2: Multi-Head Self-Attention Relation D...</td>\n",
       "      <td></td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>Wenhui Wang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2012.15828v2)</td>\n",
       "      <td>We generalize deep self-attention distillation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Model Compression</td>\n",
       "      <td>ACM SIGKDD 2006</td>\n",
       "      <td>2006-08-20</td>\n",
       "      <td>Cristian Bucil˘a et al</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/abs/10.1145/1150...</td>\n",
       "      <td>Often the best performing supervised learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Adaptive Mixtures of Local Experts</td>\n",
       "      <td>MIT Press 1991</td>\n",
       "      <td>1991-03-01</td>\n",
       "      <td>Robert A. Jacobs et al</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>We present a new supervised learning procedure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Dropout: A Simple Way to Prevent Neural Networ...</td>\n",
       "      <td>JMLR 2014</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>Nitish Srivastava et al</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>Deep neural nets with a large number of parame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Linguistic Regularities in Continuous Space Wo...</td>\n",
       "      <td>NAACL 2013</td>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>Tomas Mikolov et al</td>\n",
       "      <td>[Link](https://aclanthology.org/N13-1090/)</td>\n",
       "      <td>Continuous space language models have recently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Large-Scale Distributed Language Modeling</td>\n",
       "      <td>IEEE 2007</td>\n",
       "      <td>2007-04-05</td>\n",
       "      <td>Ahmad Emami et al</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/document/42...</td>\n",
       "      <td>A novel distributed language model that has no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BLEU: a method for automatic evaluation of mac...</td>\n",
       "      <td>ACL 2002</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>Kishore Papineni et al</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/10.3115/1073083....</td>\n",
       "      <td>Human evaluations of machine translation are e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Large Language Models in Machine Translation</td>\n",
       "      <td>EMNLP 2007</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>Thorsten Brants et al</td>\n",
       "      <td>[Link](https://aclanthology.org/D07-1090/)</td>\n",
       "      <td>This paper reports on the benefits of largesca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title Journal/Conference   \n",
       "0   Neural Machine Translation by Jointly Learning...         ICLR  2015  \\\n",
       "1   Sequence to Sequence Learning with Neural Netw...                      \n",
       "2                           Attention Is All You Need                      \n",
       "3   Google's Neural Machine Translation System: Br...                      \n",
       "4   Neural Machine Translation of Rare Words with ...          ACL  2016   \n",
       "5   Efficient Estimation of Word Representations i...                      \n",
       "6   SentencePiece: A simple and language independe...         EMNLP 2018   \n",
       "7            Deep contextualized word representations        NAACL  2018   \n",
       "8   BERT: Pre-training of Deep Bidirectional Trans...                      \n",
       "9                    Ensemble deep learning: A review                      \n",
       "10  A Survey on Model Compression and Acceleration...         AAAI  2023   \n",
       "11       Distilling the Knowledge in a Neural Network         NIPS  2014   \n",
       "12  DistilBERT, a distilled version of BERT: small...                      \n",
       "13  Patient Knowledge Distillation for BERT Model ...        EMNLP  2019   \n",
       "14  Reliable Post hoc Explanations: Modeling Uncer...                      \n",
       "15  Adaptive Computation Time for Recurrent Neural...                      \n",
       "16  BranchyNet: Fast Inference via Early Exiting f...                      \n",
       "17  GLUE: A Multi-Task Benchmark and Analysis Plat...         ICLR  2019   \n",
       "18  Improved Knowledge Distillation via Teacher As...         AAAI  2020   \n",
       "19  FastBERT: a Self-distilling BERT with Adaptive...          ACL  2020   \n",
       "20  MiniLM: Deep Self-Attention Distillation for T...                      \n",
       "21  MiniLMv2: Multi-Head Self-Attention Relation D...                      \n",
       "22                                  Model Compression    ACM SIGKDD 2006   \n",
       "23                 Adaptive Mixtures of Local Experts     MIT Press 1991   \n",
       "24  Dropout: A Simple Way to Prevent Neural Networ...          JMLR 2014   \n",
       "25  Linguistic Regularities in Continuous Space Wo...         NAACL 2013   \n",
       "26          Large-Scale Distributed Language Modeling          IEEE 2007   \n",
       "27  BLEU: a method for automatic evaluation of mac...           ACL 2002   \n",
       "28       Large Language Models in Machine Translation         EMNLP 2007   \n",
       "\n",
       "          Date                       Author   \n",
       "0   2014-09-01       Dzmitry Bahdanau et al  \\\n",
       "1   2014-09-10         Ilya Sutskever et al   \n",
       "2   2017-06-12         Ashish Vaswani et al   \n",
       "3   2016-09-26             Yonghui Wu et al   \n",
       "4   2015-08-31          Rico Sennrich et al   \n",
       "5   2013-01-16          Tomas Mikolov et al   \n",
       "6   2018-08-19              Taku Kudo et al   \n",
       "7   2018-02-15      Matthew E. Peters et al   \n",
       "8   2018-10-11           Jacob Devlin et al   \n",
       "9   2021-04-06           M. A. Ganaie et al   \n",
       "10  2022-02-15              Canwen Xu et al   \n",
       "11  2015-03-09        Geoffrey Hinton et al   \n",
       "12  2019-10-02            Victor Sanh et al   \n",
       "13  2019-08-25               Siqi Sun et al   \n",
       "14  2020-08-11            Dylan Slack et al   \n",
       "15  2016-03-29            Alex Graves et al   \n",
       "16  2017-09-06  Surat Teerapittayanon et al   \n",
       "17  2018-04-20              Alex Wang et al   \n",
       "18  2019-02-09    Seyed-Iman Mirzadeh et al   \n",
       "19  2020-04-05             Weijie Liu et al   \n",
       "20  2020-02-25            Wenhui Wang et al   \n",
       "21  2020-12-31            Wenhui Wang et al   \n",
       "22  2006-08-20       Cristian Bucil˘a et al   \n",
       "23  1991-03-01       Robert A. Jacobs et al   \n",
       "24  2014-01-01      Nitish Srivastava et al   \n",
       "25  2013-06-01          Tomas Mikolov et al   \n",
       "26  2007-04-05            Ahmad Emami et al   \n",
       "27  2002-07-01       Kishore Papineni et al   \n",
       "28  2007-06-01        Thorsten Brants et al   \n",
       "\n",
       "                                                 Link   \n",
       "0            [Link](http://arxiv.org/abs/1409.0473v7)  \\\n",
       "1            [Link](http://arxiv.org/abs/1409.3215v3)   \n",
       "2           [Link](http://arxiv.org/abs/1706.03762v5)   \n",
       "3           [Link](http://arxiv.org/abs/1609.08144v2)   \n",
       "4           [Link](http://arxiv.org/abs/1508.07909v5)   \n",
       "5            [Link](http://arxiv.org/abs/1301.3781v3)   \n",
       "6           [Link](http://arxiv.org/abs/1808.06226v1)   \n",
       "7           [Link](http://arxiv.org/abs/1802.05365v2)   \n",
       "8           [Link](http://arxiv.org/abs/1810.04805v2)   \n",
       "9           [Link](http://arxiv.org/abs/2104.02395v3)   \n",
       "10          [Link](http://arxiv.org/abs/2202.07105v2)   \n",
       "11          [Link](http://arxiv.org/abs/1503.02531v1)   \n",
       "12          [Link](http://arxiv.org/abs/1910.01108v4)   \n",
       "13          [Link](http://arxiv.org/abs/1908.09355v1)   \n",
       "14          [Link](http://arxiv.org/abs/2008.05030v4)   \n",
       "15          [Link](http://arxiv.org/abs/1603.08983v6)   \n",
       "16          [Link](http://arxiv.org/abs/1709.01686v1)   \n",
       "17          [Link](http://arxiv.org/abs/1804.07461v3)   \n",
       "18          [Link](http://arxiv.org/abs/1902.03393v2)   \n",
       "19          [Link](http://arxiv.org/abs/2004.02178v2)   \n",
       "20          [Link](http://arxiv.org/abs/2002.10957v2)   \n",
       "21          [Link](http://arxiv.org/abs/2012.15828v2)   \n",
       "22  [Link](https://dl.acm.org/doi/abs/10.1145/1150...   \n",
       "23  [Link](https://ieeexplore.ieee.org/abstract/do...   \n",
       "24  [Link](https://ieeexplore.ieee.org/abstract/do...   \n",
       "25         [Link](https://aclanthology.org/N13-1090/)   \n",
       "26  [Link](https://ieeexplore.ieee.org/document/42...   \n",
       "27  [Link](https://dl.acm.org/doi/10.3115/1073083....   \n",
       "28         [Link](https://aclanthology.org/D07-1090/)   \n",
       "\n",
       "                                             Abstract  \n",
       "0   Neural machine translation is a recently propo...  \n",
       "1   Deep Neural Networks (DNNs) are powerful model...  \n",
       "2   The dominant sequence transduction models are ...  \n",
       "3   Neural Machine Translation (NMT) is an end-to-...  \n",
       "4   Neural machine translation (NMT) models typica...  \n",
       "5   We propose two novel model architectures for c...  \n",
       "6   This paper describes SentencePiece, a language...  \n",
       "7   We introduce a new type of deep contextualized...  \n",
       "8   We introduce a new language representation mod...  \n",
       "9   Ensemble learning combines several individual ...  \n",
       "10  Despite achieving state-of-the-art performance...  \n",
       "11  A very simple way to improve the performance o...  \n",
       "12  As Transfer Learning from large-scale pre-trai...  \n",
       "13  Pre-trained language models such as BERT have ...  \n",
       "14  As black box explanations are increasingly bei...  \n",
       "15  This paper introduces Adaptive Computation Tim...  \n",
       "16  Deep neural networks are state of the art meth...  \n",
       "17  For natural language understanding (NLU) techn...  \n",
       "18  Despite the fact that deep neural networks are...  \n",
       "19  Pre-trained language models like BERT have pro...  \n",
       "20  Pre-trained language models (e.g., BERT (Devli...  \n",
       "21  We generalize deep self-attention distillation...  \n",
       "22  Often the best performing supervised learning ...  \n",
       "23  We present a new supervised learning procedure...  \n",
       "24  Deep neural nets with a large number of parame...  \n",
       "25  Continuous space language models have recently...  \n",
       "26  A novel distributed language model that has no...  \n",
       "27  Human evaluations of machine translation are e...  \n",
       "28  This paper reports on the benefits of largesca...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract.to_excel(\"arxiv_paper_df_with_abstract.xlsx\")\n",
    "arxiv_paper_df.to_excel(\"arxiv_paper_df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataframe on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Excel to Markdown Converter](https://tabletomarkdown.com/convert-spreadsheet-to-markdown/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
