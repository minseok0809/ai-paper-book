{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Paper Dataframe by Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_with_abstract(paper_ids):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame({'Title':[0],\n",
    "                               'Journal/Conference':[0],\n",
    "                               'Date':[0], \n",
    "                               'Author':[0],\n",
    "                               'Link':[0],\n",
    "                               'Abstract':[0]})\n",
    "    \n",
    "    for idx, paper_id in enumerate(paper_ids):\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(search.results())\n",
    "        \n",
    "        paper_journal_conf = re.search(r'[A-Z ]+[0-9]+[0-9]+[0-9]+[0-9]', str(paper.comment))\n",
    "        if paper_journal_conf != None:\n",
    "            paper_journal_conf = paper_journal_conf.group().strip()\n",
    "            if len(paper_journal_conf) > 4:\n",
    "                if paper_journal_conf[-4] != \" \":\n",
    "                    paper_journal_conf = paper_journal_conf[:-4] + \" \" + paper_journal_conf[-4:]\n",
    "                else:\n",
    "                    paper_journal_conf = paper_journal_conf\n",
    "            elif len(paper_journal_conf) <= 4:\n",
    "                paper_journal_conf = \"\"\n",
    "        elif paper_journal_conf == None:\n",
    "            paper_journal_conf = \"\"\n",
    "\n",
    "        arxiv_paper_df_with_abstract.loc[idx] = [paper.title, \n",
    "                                paper_journal_conf,\n",
    "                                paper.published.date(), \n",
    "                                str(paper.authors[0]) + ' et al',\n",
    "                                    paper.entry_id,\n",
    "                                    paper.summary]\n",
    "\n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_convert_datetime(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_other_papers_column(arxiv_paper_df_with_abstract, other_papers):\n",
    "  \n",
    "  df_length = len(arxiv_paper_df_with_abstract) - 1\n",
    "\n",
    "  for other_paper in other_papers:\n",
    "    df_length += 1\n",
    "    arxiv_paper_df_with_abstract.loc[df_length] = other_paper\n",
    "  \n",
    "  return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperlink(x):\n",
    "    hyperlink= '[Link]' + '(' + x + ')'\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)    \n",
    "    arxiv_paper_df = arxiv_paper_df_with_abstract.drop(['Abstract'], axis='columns')\n",
    "\n",
    "    return arxiv_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = [\"1409.0473v7\", \"1409.3215v3\", \"1706.03762v5\", \"1609.08144v2\",\n",
    "             \"1508.07909v5\", \"1301.3781v3\", \"1808.06226v1\", \"1802.05365v2\",\n",
    "             \"1810.04805v2\", \"2104.02395v3\", \"2202.07105v2\", \"1503.02531v1\",\n",
    "             \"1910.01108v4\", \"1908.09355v1\", \"2008.05030v4\", \"1603.08983v6\",\n",
    "             \"1709.01686v1\", \"1804.07461v3\", \"1902.03393v2\", \"2004.02178v2\",\n",
    "             \"2002.10957v2\", \"2012.15828v2\", \"1707.07328v1\", \"1612.00796v2\",\n",
    "             \"1806.00451v1\", \"2106.04570v3\", \"1703.03400v3\", \"1604.00289v3\"]\n",
    "\n",
    "arxiv_paper_df_with_abstract = make_arxiv_paper_df_with_abstract(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_papers = [[\"Model Compression\", \"ACM SIGKDD 2006\", str_convert_datetime(\"2006-08-20\"),\n",
    "                \"Cristian Bucil˘a et al\", \"https://dl.acm.org/doi/abs/10.1145/1150402.1150464\", \n",
    "                \"Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for 'compressing' large, complex ensembles into smaller, faster models, usually without significant loss in performance.\"],\n",
    "                [\"Adaptive Mixtures of Local Experts\", \"MIT Press 1991\", str_convert_datetime(\"1991-03-01\"),\n",
    "                \"Robert A. Jacobs et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.\"],\n",
    "                [\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", \" JMLR 2014\", str_convert_datetime(\"2014-01-01\"),\n",
    "                \"Nitish Srivastava et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\"],\n",
    "                [\"Linguistic Regularities in Continuous Space Word Representations\", \" NAACL 2013\", str_convert_datetime(\"2013-06-01\"),\n",
    "                \"Tomas Mikolov et al\", \"https://aclanthology.org/N13-1090/\", \n",
    "                \"Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.\"],\n",
    "                 [\"Large-Scale Distributed Language Modeling\", \" IEEE 2007\", str_convert_datetime(\"2007-04-05\"),\n",
    "                \"Ahmad Emami et al\", \"https://ieeexplore.ieee.org/document/4218031\", \n",
    "                \"A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.\"],\n",
    "                 [\"BLEU: a method for automatic evaluation of machine translation\", \" ACL 2002\", str_convert_datetime(\"2002-07-01\"),\n",
    "                \"Kishore Papineni et al\", \"https://dl.acm.org/doi/10.3115/1073083.1073135\", \n",
    "                \"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.\"],\n",
    "                [\"Large Language Models in Machine Translation\", \" EMNLP 2007\", str_convert_datetime(\"2007-06-01\"),\n",
    "                \"Gloria Brown Wright et al\", \"https://aclanthology.org/D07-1090/\", \n",
    "                \"This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train onup to 2 trillion tokens, resulting in languagemodels having up to 300 billion n-grams. Itis capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbedStupid Backoff, that is inexpensive to trainon large data sets and approaches the qualityof Kneser-Ney Smoothing as the amount oftraining data increases.\"],\n",
    "                [\"Student-Centered Learning in Higher Education\", \" \", str_convert_datetime(\"2011-01-01\"),\n",
    "                \"Thorsten Brants et al\", \"https://files.eric.ed.gov/fulltext/EJ938583.pdf\", \n",
    "                \"In her book, Learner-Centered Teaching, Maryellen Weimer contrasts the practices of teachercentered college teaching and student-centered college teaching in terms of (1) the balance of power in the classroom, (2) the function of the course content, (3) the role of the teacher versus the role of the student, (4) the responsibility of learning, (5) the purpose and processes of evaluation. She then gives some suggestions on how to implement the learner-centered approach. Using Weimer’s five specifications, it has been possible to identify from the pedagogical literature several examples where college teachers are seeking to move toward more student-centered classrooms. This essay reports on innovations used by teachers across the academic and professional spectrum, as well as on their evaluations of their successes.\"],\n",
    "                [\"Optimization as A Model for Few-shot Learning\", \" ICLR 2017\", str_convert_datetime(\"2017-07-22\"),\n",
    "                \"Sachin Ravi et al\", \"https://openreview.net/forum?id=rJY0-Kcll\", \n",
    "                \"Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.\"],\n",
    "                [\"Language Models are Unsupervised Multitask Learners\", \" \", str_convert_datetime(\"2019-06-01\"),\n",
    "                \"Alec Radford et al\", \"https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\", \n",
    "                \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\"]\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "arxiv_paper_df_with_abstract = add_other_papers_column(arxiv_paper_df_with_abstract, other_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract[\"Link\"] = arxiv_paper_df_with_abstract[\"Link\"].apply(hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df = make_arxiv_paper_df(arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Machine Translation by Jointly Learning...</td>\n",
       "      <td>ICLR  2015</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>Dzmitry Bahdanau et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1409.0473v7)</td>\n",
       "      <td>Neural machine translation is a recently propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td></td>\n",
       "      <td>2014-09-10</td>\n",
       "      <td>Ilya Sutskever et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1409.3215v3)</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td></td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>Ashish Vaswani et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1706.03762v5)</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google's Neural Machine Translation System: Br...</td>\n",
       "      <td></td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>Yonghui Wu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1609.08144v2)</td>\n",
       "      <td>Neural Machine Translation (NMT) is an end-to-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Machine Translation of Rare Words with ...</td>\n",
       "      <td>ACL  2016</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Rico Sennrich et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1508.07909v5)</td>\n",
       "      <td>Neural machine translation (NMT) models typica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Efficient Estimation of Word Representations i...</td>\n",
       "      <td></td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>Tomas Mikolov et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1301.3781v3)</td>\n",
       "      <td>We propose two novel model architectures for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SentencePiece: A simple and language independe...</td>\n",
       "      <td>EMNLP 2018</td>\n",
       "      <td>2018-08-19</td>\n",
       "      <td>Taku Kudo et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1808.06226v1)</td>\n",
       "      <td>This paper describes SentencePiece, a language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>NAACL  2018</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>Matthew E. Peters et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1802.05365v2)</td>\n",
       "      <td>We introduce a new type of deep contextualized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>Jacob Devlin et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1810.04805v2)</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ensemble deep learning: A review</td>\n",
       "      <td></td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>M. A. Ganaie et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2104.02395v3)</td>\n",
       "      <td>Ensemble learning combines several individual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A Survey on Model Compression and Acceleration...</td>\n",
       "      <td>AAAI  2023</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>Canwen Xu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2202.07105v2)</td>\n",
       "      <td>Despite achieving state-of-the-art performance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Distilling the Knowledge in a Neural Network</td>\n",
       "      <td>NIPS  2014</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>Geoffrey Hinton et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1503.02531v1)</td>\n",
       "      <td>A very simple way to improve the performance o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>Victor Sanh et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1910.01108v4)</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Patient Knowledge Distillation for BERT Model ...</td>\n",
       "      <td>EMNLP  2019</td>\n",
       "      <td>2019-08-25</td>\n",
       "      <td>Siqi Sun et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1908.09355v1)</td>\n",
       "      <td>Pre-trained language models such as BERT have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Reliable Post hoc Explanations: Modeling Uncer...</td>\n",
       "      <td></td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>Dylan Slack et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2008.05030v4)</td>\n",
       "      <td>As black box explanations are increasingly bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Adaptive Computation Time for Recurrent Neural...</td>\n",
       "      <td></td>\n",
       "      <td>2016-03-29</td>\n",
       "      <td>Alex Graves et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1603.08983v6)</td>\n",
       "      <td>This paper introduces Adaptive Computation Tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BranchyNet: Fast Inference via Early Exiting f...</td>\n",
       "      <td></td>\n",
       "      <td>2017-09-06</td>\n",
       "      <td>Surat Teerapittayanon et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1709.01686v1)</td>\n",
       "      <td>Deep neural networks are state of the art meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GLUE: A Multi-Task Benchmark and Analysis Plat...</td>\n",
       "      <td>ICLR  2019</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>Alex Wang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1804.07461v3)</td>\n",
       "      <td>For natural language understanding (NLU) techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Improved Knowledge Distillation via Teacher As...</td>\n",
       "      <td>AAAI  2020</td>\n",
       "      <td>2019-02-09</td>\n",
       "      <td>Seyed-Iman Mirzadeh et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1902.03393v2)</td>\n",
       "      <td>Despite the fact that deep neural networks are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FastBERT: a Self-distilling BERT with Adaptive...</td>\n",
       "      <td>ACL  2020</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>Weijie Liu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2004.02178v2)</td>\n",
       "      <td>Pre-trained language models like BERT have pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MiniLM: Deep Self-Attention Distillation for T...</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>Wenhui Wang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2002.10957v2)</td>\n",
       "      <td>Pre-trained language models (e.g., BERT (Devli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MiniLMv2: Multi-Head Self-Attention Relation D...</td>\n",
       "      <td></td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>Wenhui Wang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2012.15828v2)</td>\n",
       "      <td>We generalize deep self-attention distillation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Adversarial Examples for Evaluating Reading Co...</td>\n",
       "      <td>EMNLP  2017</td>\n",
       "      <td>2017-07-23</td>\n",
       "      <td>Robin Jia et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1707.07328v1)</td>\n",
       "      <td>Standard accuracy metrics indicate that readin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Overcoming catastrophic forgetting in neural n...</td>\n",
       "      <td></td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>James Kirkpatrick et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1612.00796v2)</td>\n",
       "      <td>The ability to learn tasks in a sequential fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Do CIFAR-10 Classifiers Generalize to CIFAR-10?</td>\n",
       "      <td></td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>Benjamin Recht et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1806.00451v1)</td>\n",
       "      <td>Machine learning is currently dominated by lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BERT Learns to Teach: Knowledge Distillation w...</td>\n",
       "      <td>ACL  2022</td>\n",
       "      <td>2021-06-08</td>\n",
       "      <td>Wangchunshu Zhou et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2106.04570v3)</td>\n",
       "      <td>We present Knowledge Distillation with Meta Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Model-Agnostic Meta-Learning for Fast Adaptati...</td>\n",
       "      <td>ICML  2017</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>Chelsea Finn et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1703.03400v3)</td>\n",
       "      <td>We propose an algorithm for meta-learning that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Building Machines That Learn and Think Like Pe...</td>\n",
       "      <td></td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Brenden M. Lake et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1604.00289v3)</td>\n",
       "      <td>Recent progress in artificial intelligence (AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Model Compression</td>\n",
       "      <td>ACM SIGKDD 2006</td>\n",
       "      <td>2006-08-20</td>\n",
       "      <td>Cristian Bucil˘a et al</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/abs/10.1145/1150...</td>\n",
       "      <td>Often the best performing supervised learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Adaptive Mixtures of Local Experts</td>\n",
       "      <td>MIT Press 1991</td>\n",
       "      <td>1991-03-01</td>\n",
       "      <td>Robert A. Jacobs et al</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>We present a new supervised learning procedure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Dropout: A Simple Way to Prevent Neural Networ...</td>\n",
       "      <td>JMLR 2014</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>Nitish Srivastava et al</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>Deep neural nets with a large number of parame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Linguistic Regularities in Continuous Space Wo...</td>\n",
       "      <td>NAACL 2013</td>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>Tomas Mikolov et al</td>\n",
       "      <td>[Link](https://aclanthology.org/N13-1090/)</td>\n",
       "      <td>Continuous space language models have recently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Large-Scale Distributed Language Modeling</td>\n",
       "      <td>IEEE 2007</td>\n",
       "      <td>2007-04-05</td>\n",
       "      <td>Ahmad Emami et al</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/document/42...</td>\n",
       "      <td>A novel distributed language model that has no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BLEU: a method for automatic evaluation of mac...</td>\n",
       "      <td>ACL 2002</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>Kishore Papineni et al</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/10.3115/1073083....</td>\n",
       "      <td>Human evaluations of machine translation are e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Large Language Models in Machine Translation</td>\n",
       "      <td>EMNLP 2007</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>Gloria Brown Wright et al</td>\n",
       "      <td>[Link](https://aclanthology.org/D07-1090/)</td>\n",
       "      <td>This paper reports on the benefits of largesca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Student-Centered Learning in Higher Education</td>\n",
       "      <td></td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>Thorsten Brants et al</td>\n",
       "      <td>[Link](https://files.eric.ed.gov/fulltext/EJ93...</td>\n",
       "      <td>In her book, Learner-Centered Teaching, Maryel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Optimization as A Model for Few-shot Learning</td>\n",
       "      <td>ICLR 2017</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>Sachin Ravi et al</td>\n",
       "      <td>[Link](https://openreview.net/forum?id=rJY0-Kcll)</td>\n",
       "      <td>Though deep neural networks have shown great s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Language Models are Unsupervised Multitask Lea...</td>\n",
       "      <td></td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>Alec Radford et al</td>\n",
       "      <td>[Link](https://www.semanticscholar.org/paper/L...</td>\n",
       "      <td>Natural language processing tasks, such as que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title Journal/Conference   \n",
       "0   Neural Machine Translation by Jointly Learning...         ICLR  2015  \\\n",
       "1   Sequence to Sequence Learning with Neural Netw...                      \n",
       "2                           Attention Is All You Need                      \n",
       "3   Google's Neural Machine Translation System: Br...                      \n",
       "4   Neural Machine Translation of Rare Words with ...          ACL  2016   \n",
       "5   Efficient Estimation of Word Representations i...                      \n",
       "6   SentencePiece: A simple and language independe...         EMNLP 2018   \n",
       "7            Deep contextualized word representations        NAACL  2018   \n",
       "8   BERT: Pre-training of Deep Bidirectional Trans...                      \n",
       "9                    Ensemble deep learning: A review                      \n",
       "10  A Survey on Model Compression and Acceleration...         AAAI  2023   \n",
       "11       Distilling the Knowledge in a Neural Network         NIPS  2014   \n",
       "12  DistilBERT, a distilled version of BERT: small...                      \n",
       "13  Patient Knowledge Distillation for BERT Model ...        EMNLP  2019   \n",
       "14  Reliable Post hoc Explanations: Modeling Uncer...                      \n",
       "15  Adaptive Computation Time for Recurrent Neural...                      \n",
       "16  BranchyNet: Fast Inference via Early Exiting f...                      \n",
       "17  GLUE: A Multi-Task Benchmark and Analysis Plat...         ICLR  2019   \n",
       "18  Improved Knowledge Distillation via Teacher As...         AAAI  2020   \n",
       "19  FastBERT: a Self-distilling BERT with Adaptive...          ACL  2020   \n",
       "20  MiniLM: Deep Self-Attention Distillation for T...                      \n",
       "21  MiniLMv2: Multi-Head Self-Attention Relation D...                      \n",
       "22  Adversarial Examples for Evaluating Reading Co...        EMNLP  2017   \n",
       "23  Overcoming catastrophic forgetting in neural n...                      \n",
       "24    Do CIFAR-10 Classifiers Generalize to CIFAR-10?                      \n",
       "25  BERT Learns to Teach: Knowledge Distillation w...          ACL  2022   \n",
       "26  Model-Agnostic Meta-Learning for Fast Adaptati...         ICML  2017   \n",
       "27  Building Machines That Learn and Think Like Pe...                      \n",
       "28                                  Model Compression    ACM SIGKDD 2006   \n",
       "29                 Adaptive Mixtures of Local Experts     MIT Press 1991   \n",
       "30  Dropout: A Simple Way to Prevent Neural Networ...          JMLR 2014   \n",
       "31  Linguistic Regularities in Continuous Space Wo...         NAACL 2013   \n",
       "32          Large-Scale Distributed Language Modeling          IEEE 2007   \n",
       "33  BLEU: a method for automatic evaluation of mac...           ACL 2002   \n",
       "34       Large Language Models in Machine Translation         EMNLP 2007   \n",
       "35      Student-Centered Learning in Higher Education                      \n",
       "36      Optimization as A Model for Few-shot Learning          ICLR 2017   \n",
       "37  Language Models are Unsupervised Multitask Lea...                      \n",
       "\n",
       "          Date                       Author   \n",
       "0   2014-09-01       Dzmitry Bahdanau et al  \\\n",
       "1   2014-09-10         Ilya Sutskever et al   \n",
       "2   2017-06-12         Ashish Vaswani et al   \n",
       "3   2016-09-26             Yonghui Wu et al   \n",
       "4   2015-08-31          Rico Sennrich et al   \n",
       "5   2013-01-16          Tomas Mikolov et al   \n",
       "6   2018-08-19              Taku Kudo et al   \n",
       "7   2018-02-15      Matthew E. Peters et al   \n",
       "8   2018-10-11           Jacob Devlin et al   \n",
       "9   2021-04-06           M. A. Ganaie et al   \n",
       "10  2022-02-15              Canwen Xu et al   \n",
       "11  2015-03-09        Geoffrey Hinton et al   \n",
       "12  2019-10-02            Victor Sanh et al   \n",
       "13  2019-08-25               Siqi Sun et al   \n",
       "14  2020-08-11            Dylan Slack et al   \n",
       "15  2016-03-29            Alex Graves et al   \n",
       "16  2017-09-06  Surat Teerapittayanon et al   \n",
       "17  2018-04-20              Alex Wang et al   \n",
       "18  2019-02-09    Seyed-Iman Mirzadeh et al   \n",
       "19  2020-04-05             Weijie Liu et al   \n",
       "20  2020-02-25            Wenhui Wang et al   \n",
       "21  2020-12-31            Wenhui Wang et al   \n",
       "22  2017-07-23              Robin Jia et al   \n",
       "23  2016-12-02      James Kirkpatrick et al   \n",
       "24  2018-06-01         Benjamin Recht et al   \n",
       "25  2021-06-08       Wangchunshu Zhou et al   \n",
       "26  2017-03-09           Chelsea Finn et al   \n",
       "27  2016-04-01        Brenden M. Lake et al   \n",
       "28  2006-08-20       Cristian Bucil˘a et al   \n",
       "29  1991-03-01       Robert A. Jacobs et al   \n",
       "30  2014-01-01      Nitish Srivastava et al   \n",
       "31  2013-06-01          Tomas Mikolov et al   \n",
       "32  2007-04-05            Ahmad Emami et al   \n",
       "33  2002-07-01       Kishore Papineni et al   \n",
       "34  2007-06-01    Gloria Brown Wright et al   \n",
       "35  2011-01-01        Thorsten Brants et al   \n",
       "36  2017-07-22            Sachin Ravi et al   \n",
       "37  2019-06-01           Alec Radford et al   \n",
       "\n",
       "                                                 Link   \n",
       "0            [Link](http://arxiv.org/abs/1409.0473v7)  \\\n",
       "1            [Link](http://arxiv.org/abs/1409.3215v3)   \n",
       "2           [Link](http://arxiv.org/abs/1706.03762v5)   \n",
       "3           [Link](http://arxiv.org/abs/1609.08144v2)   \n",
       "4           [Link](http://arxiv.org/abs/1508.07909v5)   \n",
       "5            [Link](http://arxiv.org/abs/1301.3781v3)   \n",
       "6           [Link](http://arxiv.org/abs/1808.06226v1)   \n",
       "7           [Link](http://arxiv.org/abs/1802.05365v2)   \n",
       "8           [Link](http://arxiv.org/abs/1810.04805v2)   \n",
       "9           [Link](http://arxiv.org/abs/2104.02395v3)   \n",
       "10          [Link](http://arxiv.org/abs/2202.07105v2)   \n",
       "11          [Link](http://arxiv.org/abs/1503.02531v1)   \n",
       "12          [Link](http://arxiv.org/abs/1910.01108v4)   \n",
       "13          [Link](http://arxiv.org/abs/1908.09355v1)   \n",
       "14          [Link](http://arxiv.org/abs/2008.05030v4)   \n",
       "15          [Link](http://arxiv.org/abs/1603.08983v6)   \n",
       "16          [Link](http://arxiv.org/abs/1709.01686v1)   \n",
       "17          [Link](http://arxiv.org/abs/1804.07461v3)   \n",
       "18          [Link](http://arxiv.org/abs/1902.03393v2)   \n",
       "19          [Link](http://arxiv.org/abs/2004.02178v2)   \n",
       "20          [Link](http://arxiv.org/abs/2002.10957v2)   \n",
       "21          [Link](http://arxiv.org/abs/2012.15828v2)   \n",
       "22          [Link](http://arxiv.org/abs/1707.07328v1)   \n",
       "23          [Link](http://arxiv.org/abs/1612.00796v2)   \n",
       "24          [Link](http://arxiv.org/abs/1806.00451v1)   \n",
       "25          [Link](http://arxiv.org/abs/2106.04570v3)   \n",
       "26          [Link](http://arxiv.org/abs/1703.03400v3)   \n",
       "27          [Link](http://arxiv.org/abs/1604.00289v3)   \n",
       "28  [Link](https://dl.acm.org/doi/abs/10.1145/1150...   \n",
       "29  [Link](https://ieeexplore.ieee.org/abstract/do...   \n",
       "30  [Link](https://ieeexplore.ieee.org/abstract/do...   \n",
       "31         [Link](https://aclanthology.org/N13-1090/)   \n",
       "32  [Link](https://ieeexplore.ieee.org/document/42...   \n",
       "33  [Link](https://dl.acm.org/doi/10.3115/1073083....   \n",
       "34         [Link](https://aclanthology.org/D07-1090/)   \n",
       "35  [Link](https://files.eric.ed.gov/fulltext/EJ93...   \n",
       "36  [Link](https://openreview.net/forum?id=rJY0-Kcll)   \n",
       "37  [Link](https://www.semanticscholar.org/paper/L...   \n",
       "\n",
       "                                             Abstract  \n",
       "0   Neural machine translation is a recently propo...  \n",
       "1   Deep Neural Networks (DNNs) are powerful model...  \n",
       "2   The dominant sequence transduction models are ...  \n",
       "3   Neural Machine Translation (NMT) is an end-to-...  \n",
       "4   Neural machine translation (NMT) models typica...  \n",
       "5   We propose two novel model architectures for c...  \n",
       "6   This paper describes SentencePiece, a language...  \n",
       "7   We introduce a new type of deep contextualized...  \n",
       "8   We introduce a new language representation mod...  \n",
       "9   Ensemble learning combines several individual ...  \n",
       "10  Despite achieving state-of-the-art performance...  \n",
       "11  A very simple way to improve the performance o...  \n",
       "12  As Transfer Learning from large-scale pre-trai...  \n",
       "13  Pre-trained language models such as BERT have ...  \n",
       "14  As black box explanations are increasingly bei...  \n",
       "15  This paper introduces Adaptive Computation Tim...  \n",
       "16  Deep neural networks are state of the art meth...  \n",
       "17  For natural language understanding (NLU) techn...  \n",
       "18  Despite the fact that deep neural networks are...  \n",
       "19  Pre-trained language models like BERT have pro...  \n",
       "20  Pre-trained language models (e.g., BERT (Devli...  \n",
       "21  We generalize deep self-attention distillation...  \n",
       "22  Standard accuracy metrics indicate that readin...  \n",
       "23  The ability to learn tasks in a sequential fas...  \n",
       "24  Machine learning is currently dominated by lar...  \n",
       "25  We present Knowledge Distillation with Meta Le...  \n",
       "26  We propose an algorithm for meta-learning that...  \n",
       "27  Recent progress in artificial intelligence (AI...  \n",
       "28  Often the best performing supervised learning ...  \n",
       "29  We present a new supervised learning procedure...  \n",
       "30  Deep neural nets with a large number of parame...  \n",
       "31  Continuous space language models have recently...  \n",
       "32  A novel distributed language model that has no...  \n",
       "33  Human evaluations of machine translation are e...  \n",
       "34  This paper reports on the benefits of largesca...  \n",
       "35  In her book, Learner-Centered Teaching, Maryel...  \n",
       "36  Though deep neural networks have shown great s...  \n",
       "37  Natural language processing tasks, such as que...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract.to_excel(\"arxiv_paper_df_with_abstract.xlsx\")\n",
    "arxiv_paper_df.to_excel(\"arxiv_paper_df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataframe on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Excel to Markdown Converter](https://tabletomarkdown.com/convert-spreadsheet-to-markdown/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
