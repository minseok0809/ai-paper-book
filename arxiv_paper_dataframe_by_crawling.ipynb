{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Paper Dataframe by Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install arxiv\n",
    "%pip install clipboard\n",
    "%pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import arxiv\n",
    "import clipboard\n",
    "import pyautogui\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_with_abstract(paper_ids):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame({'Title':['Noun'],\n",
    "                               'Journal/Conference':['Noun'],\n",
    "                               'Date':['Noun'], \n",
    "                               'Author':['Noun'],\n",
    "                               'Link':['Noun'],\n",
    "                               'Abstract':['Noun']})\n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    pbar = tqdm.tqdm(paper_ids)\n",
    "\n",
    "    for idx, paper_id in enumerate(pbar):\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(client.results(search))\n",
    "        \n",
    "        paper_journal_conf = re.search(r'[A-Z ]+[0-9]+[0-9]+[0-9]+[0-9]', str(paper.comment))\n",
    "        if paper_journal_conf != None:\n",
    "            paper_journal_conf = paper_journal_conf.group().strip()\n",
    "            if len(paper_journal_conf) > 4:\n",
    "                if paper_journal_conf[-4] != \" \":\n",
    "                    paper_journal_conf = paper_journal_conf[:-4] + \" \" + paper_journal_conf[-4:]\n",
    "                else:\n",
    "                    paper_journal_conf = paper_journal_conf\n",
    "            elif len(paper_journal_conf) <= 4:\n",
    "                paper_journal_conf = \"\"\n",
    "        elif paper_journal_conf == None:\n",
    "            paper_journal_conf = \"\"\n",
    "\n",
    "        arxiv_paper_df_with_abstract.loc[idx] = [paper.title, \n",
    "                                paper_journal_conf,\n",
    "                                paper.published.date(), \n",
    "                                str(paper.authors[0]) + ' et al',\n",
    "                                    paper.entry_id,\n",
    "                                    paper.summary]\n",
    "    pbar.close()\n",
    "    \n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)         \n",
    "    \n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_convert_datetime(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_other_papers_column(arxiv_paper_df_with_abstract, other_papers):\n",
    "  \n",
    "  df_length = len(arxiv_paper_df_with_abstract) - 1\n",
    "\n",
    "  pbar = tqdm.tqdm(other_papers)\n",
    "\n",
    "  for other_paper in pbar:\n",
    "    df_length += 1\n",
    "    arxiv_paper_df_with_abstract.loc[df_length] = other_paper\n",
    "  \n",
    "  arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "  arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)  \n",
    "  \n",
    "  pbar.close()\n",
    "  \n",
    "  return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperlink(x):\n",
    "    hyperlink= '[Link]' + '(' + x + ')'\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_jouranl_conference_theme(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    paper_title = arxiv_paper_df_with_abstract['Title']\n",
    "    paper_journal_conference = arxiv_paper_df_with_abstract['Journal/Conference']\n",
    "    arxiv_paper_df_with_abstract['Theme'] = \"\"\n",
    "    paper_theme = arxiv_paper_df_with_abstract['Theme']\n",
    "\n",
    "    pyautogui.alert('Input Paper Jouranl Conference')\n",
    "\n",
    "    for index, (title, journal_conference) in enumerate(zip(paper_title, paper_journal_conference)):\n",
    "\n",
    "        if len(journal_conference) < 2:\n",
    "            clipboard.copy(title)\n",
    "            input_journal_conference = input(\"{} For {}: \".format(\"Input Journal & Conference\", title)) \n",
    "            arxiv_paper_df_with_abstract.loc[index, 'Journal/Conference'] = input_journal_conference\n",
    "\n",
    "    pyautogui.alert('Input Paper Title')\n",
    "\n",
    "    for index, (title, theme) in enumerate(zip(paper_title, paper_theme)):\n",
    "\n",
    "        if len(theme) < 2:\n",
    "            clipboard.copy(title)\n",
    "            input_theme = input(\"{} For {}: \".format(\"Input Theme\", title)) \n",
    "            arxiv_paper_df_with_abstract.loc[index, 'Theme'] = input_theme\n",
    "\n",
    "    arxiv_paper_df_with_abstract = arxiv_paper_df_with_abstract[['Title', 'Journal/Conference', 'Date', 'Author', 'Theme', 'Link', 'Abstract']]\n",
    "\n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_with_abstract_by_theme(theme_order, arxiv_paper_df_with_abstract):\n",
    "\n",
    "    def sorter(column):\n",
    "        mapper = {name: order for order, name in enumerate(theme_order)}\n",
    "        return column.map(mapper)\n",
    "\n",
    "    arxiv_paper_df_with_abstract_by_theme = arxiv_paper_df_with_abstract.sort_values(by=['Theme', 'Date'], key=sorter, ascending=True).reset_index() \n",
    "    del arxiv_paper_df_with_abstract_by_theme['index']\n",
    "    arxiv_paper_df_with_abstract_by_theme.index += 1 \n",
    "    arxiv_paper_df_with_abstract_by_theme = arxiv_paper_df_with_abstract_by_theme.set_index('Theme', append=True).swaplevel(0, 1)\n",
    "\n",
    "    return arxiv_paper_df_with_abstract_by_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)    \n",
    "    arxiv_paper_df = arxiv_paper_df_with_abstract.drop(['Abstract'], axis='columns')\n",
    "\n",
    "    return arxiv_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_by_theme(arxiv_paper_df_with_abstract_by_theme):\n",
    "  \n",
    "    arxiv_paper_df_by_theme = arxiv_paper_df_with_abstract_by_theme.drop(['Abstract'], axis='columns')\n",
    "\n",
    "    return arxiv_paper_df_by_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [07:48<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "paper_ids = [\"1409.0473v7\", \"1409.3215v3\", \"1706.03762v5\", \"1609.08144v2\",\n",
    "             \"1508.07909v5\", \"1301.3781v3\", \"1808.06226v1\", \"1802.05365v2\",\n",
    "             \"1810.04805v2\", \"2104.02395v3\", \"2202.07105v2\", \"1503.02531v1\",\n",
    "             \"1910.01108v4\", \"1908.09355v1\", \"2008.05030v4\", \"1603.08983v6\",\n",
    "             \"1709.01686v1\", \"1804.07461v3\", \"1902.03393v2\", \"2004.02178v2\",\n",
    "             \"2002.10957v2\", \"2012.15828v2\", \"1707.07328v1\", \"1612.00796v2\",\n",
    "             \"1806.00451v1\", \"2106.04570v3\", \"1703.03400v3\", \"1604.00289v3\",\n",
    "             \"2004.09602v1\", \"1603.01025v2\", \"1308.3432v1\", \"1909.05840v2\",\n",
    "             \"1712.05877v1\", \"1811.08886v3\", \"1502.03044v3\", \"1907.05686v5\", \n",
    "             \"2203.06390v1\", \"1609.07061v1\", \"1911.09464v2\", \"2103.13630v3\",\n",
    "             \"1603.05279v4\", \"1906.05714v1\", \"1905.03197v3\", \"1907.10529v3\",\n",
    "             \"1910.13461v1\", \"2007.10760v3\", \"2008.00312\", \"1803.06535\",\n",
    "             \"1705.09655\", \"1312.6114\", \"1606.03657\", \"2103.04264\",\n",
    "             \"1902.06531\", \"2004.06660\", \"1908.10084\", \"1905.10447\",\n",
    "             \"2006.01043\", \"1408.5882\", \"2301.10602\", \"1810.10191\",\n",
    "             \"1509.02971\", \"1802.09464\", \"2109.11234\", \"1707.06347\",\n",
    "             \"2006.01043\", \"1408.5882\", \"2301.10602\", \"1810.10191\",\n",
    "             \"1710.02298\", \"1312.5602\", \"1712.00378\", \"2210.03992\",\n",
    "             \"2305.17416\", \"1705.00106\", \"1806.03822\", \"1910.10683\",\n",
    "             \"2109.01652\", \"2210.03992\", \"2212.10560\", \"2302.13971\",\n",
    "             \"1909.06951\", \"1705.00106\", \"1806.03822\", \"1910.10683\",\n",
    "             \"2109.01652\", \"2210.03992\", \"2212.10560\", \"2302.13971\",\n",
    "             \"2304.03277\", \"2305.11206\", \"2305.14314\", \"2305.14152\",\n",
    "             \"2204.07705\", \"2305.17002\", \"2302.02210\", \"2203.11086\",\n",
    "             \"2010.11929\", \"2012.12877\", \"1606.06160\", \"2211.16056\",\n",
    "             \"2003.03488\", \"2004.03333\", \"2106.08295\", \"2101.01321\",\n",
    "             \"2004.04136\", \"2305.15077\", \"2106.07345\", \"2104.08821\",\n",
    "             \"1607.00325\", \"2110.06296\", \"2010.15703\", \"1711.06077\",\n",
    "             \"1903.06733\", \"2006.04884\", \"1804.00247\", \"1706.02677\",\n",
    "             \"2004.08249\", \"1711.00489\", \"1803.09820\", \"2012.15701\",\n",
    "             \"2205.13016\", \"1902.08153\", \"1603.05279\", \"2206.01861\",\n",
    "             \"2209.13325\", \"2304.09145\", \"2301.12017\", \"2306.00317\",\n",
    "             \"2305.17888\", \"1902.08153\", \"1603.05279\", \"2206.01861\",\n",
    "             \"2209.13325\", \"1911.12491\"]\n",
    "\n",
    "arxiv_paper_df_with_abstract = make_arxiv_paper_df_with_abstract(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 785.80it/s]\n"
     ]
    }
   ],
   "source": [
    "other_papers = [[\"Model Compression\", \"ACM SIGKDD 2006\", str_convert_datetime(\"2006-08-20\"),\n",
    "                \"Cristian Bucil˘a et al\", \"https://dl.acm.org/doi/abs/10.1145/1150402.1150464\", \n",
    "                \"Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for 'compressing' large, complex ensembles into smaller, faster models, usually without significant loss in performance.\"],\n",
    "                [\"Adaptive Mixtures of Local Experts\", \"MIT Press 1991\", str_convert_datetime(\"1991-03-01\"),\n",
    "                \"Robert A. Jacobs et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.\"],\n",
    "                [\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", \"JMLR 2014\", str_convert_datetime(\"2014-01-01\"),\n",
    "                \"Nitish Srivastava et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\"],\n",
    "                [\"Linguistic Regularities in Continuous Space Word Representations\", \"NAACL 2013\", str_convert_datetime(\"2013-06-01\"),\n",
    "                \"Tomas Mikolov et al\", \"https://aclanthology.org/N13-1090/\", \n",
    "                \"Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.\"],\n",
    "                 [\"Large-Scale Distributed Language Modeling\", \"IEEE 2007\", str_convert_datetime(\"2007-04-05\"),\n",
    "                \"Ahmad Emami et al\", \"https://ieeexplore.ieee.org/document/4218031\", \n",
    "                \"A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.\"],\n",
    "                 [\"BLEU: a method for automatic evaluation of machine translation\", \"ACL 2002\", str_convert_datetime(\"2002-07-01\"),\n",
    "                \"Kishore Papineni et al\", \"https://dl.acm.org/doi/10.3115/1073083.1073135\", \n",
    "                \"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.\"],\n",
    "                [\"Large Language Models in Machine Translation\", \"EMNLP 2007\", str_convert_datetime(\"2007-06-01\"),\n",
    "                \"Gloria Brown Wright et al\", \"https://aclanthology.org/D07-1090/\", \n",
    "                \"This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train onup to 2 trillion tokens, resulting in languagemodels having up to 300 billion n-grams. Itis capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbedStupid Backoff, that is inexpensive to trainon large data sets and approaches the qualityof Kneser-Ney Smoothing as the amount oftraining data increases.\"],\n",
    "                [\"Student-Centered Learning in Higher Education\", \" \", str_convert_datetime(\"2011-01-01\"),\n",
    "                \"Thorsten Brants et al\", \"https://files.eric.ed.gov/fulltext/EJ938583.pdf\", \n",
    "                \"In her book, Learner-Centered Teaching, Maryellen Weimer contrasts the practices of teachercentered college teaching and student-centered college teaching in terms of (1) the balance of power in the classroom, (2) the function of the course content, (3) the role of the teacher versus the role of the student, (4) the responsibility of learning, (5) the purpose and processes of evaluation. She then gives some suggestions on how to implement the learner-centered approach. Using Weimer’s five specifications, it has been possible to identify from the pedagogical literature several examples where college teachers are seeking to move toward more student-centered classrooms. This essay reports on innovations used by teachers across the academic and professional spectrum, as well as on their evaluations of their successes.\"],\n",
    "                [\"Optimization as A Model for Few-shot Learning\", \"ICLR 2017\", str_convert_datetime(\"2017-07-22\"),\n",
    "                \"Sachin Ravi et al\", \"https://openreview.net/forum?id=rJY0-Kcll\", \n",
    "                \"Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.\"],\n",
    "                [\"Language Models are Unsupervised Multitask Learners\", \" \", str_convert_datetime(\"2019-06-01\"),\n",
    "                \"Alec Radford et al\", \"https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\", \n",
    "                \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\"],\n",
    "                [\"BadNets: Evaluating Backdooring Attacks on Deep Neural Networks\", \"IEEE Access 2019\", str_convert_datetime(\"2019-04-11\"),\n",
    "                \"Tianyu Gu et al\", \"https://ieeexplore.ieee.org/document/8685687\", \n",
    "                \"Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks.\"],\n",
    "                [\"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks\", \"IEEE Symposium on Security and Privacy (SP) 2019\", str_convert_datetime(\"2019-01-01\"),\n",
    "                \"Bolun Wang et al\", \"https://www.computer.org/csdl/proceedings-article/sp/2019/666000a707/1dlwir1mwFi\", \n",
    "                \"Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classification to produce unexpected results.\"],\n",
    "                [\"SoMoGym: A Toolkit for Developing and Evaluating Controllers and Reinforcement Learning Algorithms for Soft Robots\", \"IEEE Robotics and Automation Letters 2022\", str_convert_datetime(\"2022-01-01\"),\n",
    "                \"Moritz A. Graule et al\", \"https://ieeexplore.ieee.org/document/9707663\", \n",
    "                \"Soft robots offer a host of benefits over traditional rigid robots, including inherent compliance that lets them passively adapt to variable environments and operate safely around humans and fragile objects.\"],\n",
    "                [\"Goal Density based Hindsight Experience Prioritization for Multi Goal Robot Manipulation Reinforcement Learning\", \"IEEE International Workshop on Robot and Human Communication (ROMAN) 2020\", str_convert_datetime(\"2020-09-04\"),\n",
    "                \"Yingyi Kuang et al\", \"https://ieeexplore.ieee.org/document/9223473\", \n",
    "                \"Reinforcement learning for multi-goal robot manipulation tasks is usually challenging, especially when sparse rewards are provided.\"],\n",
    "                [\"Augmenting Vision-Based Grasp Plans for Soft Robotic Grippers using Reinforcement Learning\", \"IEEE 18th International Conference on Automation Science and Engineering (CASE) 2022\", str_convert_datetime(\"2022-08-24\"),\n",
    "                \"Vighnesh Vatsal et al\", \"https://ieeexplore.ieee.org/document/9926580\", \n",
    "                \"Vision-based techniques for grasp planning of robotic end-effectors have been successfully deployed in pick-and-place tasks.\"],\n",
    "                [\"Learning-Based Slip Detection for Robotic Fruit Grasping and Manipulation under Leaf Interference\", \"AI-Based Sensors and Sensing Systems for Smart Agriculture) 2022\", str_convert_datetime(\"2022-06-01\"),\n",
    "                \"Hongyu Zhou et al\", \"https://www.mdpi.com/1424-8220/22/15/5483\", \n",
    "                \"Robotic harvesting research has seen significant achievements in the past decade, with breakthroughs being made in machine vision, robot manipulation, autonomous navigation and mapping.\"],\n",
    "                [\"Reward is enough\", \"Artificial Intelligenc Volume 299\", str_convert_datetime(\"2021-10-01\"),\n",
    "                \"David Silver et al\", \"https://www.sciencedirect.com/science/article/pii/S0004370221000862\", \n",
    "                \"In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward.\"],\n",
    "                [\"Hierarchical Attention Networks for Document Classification\", \"ACL 2016\", str_convert_datetime(\"2016-01-01\"),\n",
    "                \"Zichao Yang et al\", \"https://aclanthology.org/N16-1174/\", \n",
    "                \"We propose a hierarchical attention network for document classification.\"],\n",
    "                [\"ROUGE: A Package for Automatic Evaluation of Summaries\", \"Artificial Intelligenc Volume 299\", str_convert_datetime(\"2021-10-01\"),\n",
    "                \"Chin-Yew Lin et al\", \"https://aclanthology.org/W04-1013/\", \n",
    "                \"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.\"],\n",
    "                [\"Post-Training with Interrogative Sentences for Enhancing BART-based Korean Question Generator\", \"ACL 2022\", str_convert_datetime(\"2022-01-01\"),\n",
    "                \"Gyu-Min Park et al\", \"https://aclanthology.org/2022.aacl-short.26/\", \n",
    "                \"The pre-trained language models such as KoBART often fail in generating perfect interrogative sentences when they are applied to Korean question generation.\"],\n",
    "                [\"Dialog-Post Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training\", \"ACL 2023\", str_convert_datetime(\"2023-01-01\"),\n",
    "                \"Zhenyu Zhang et al\", \"https://aclanthology.org/2023.acl-long.564/\", \n",
    "                \"Dialogue representation and understanding aim to convert conversational inputs into embeddings and fulfill discriminative tasks.\"],\n",
    "                [\"Improving Transformer Optimization Through Better Initialization\", \"ICML 2020\", str_convert_datetime(\"2020-01-01\"),\n",
    "                \"Xiao Shi Huang et al\", \"https://proceedings.mlr.press/v119/huang20f.html\", \n",
    "                \"We propose a hierarchical attention network for document classification.\"]\n",
    "                ]\n",
    "\n",
    "arxiv_paper_df_with_abstract = add_other_papers_column(arxiv_paper_df_with_abstract, other_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract[\"Link\"] = arxiv_paper_df_with_abstract[\"Link\"].apply(hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract = input_jouranl_conference_theme(arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_order = [\"Neural Network\", \"Benchmark\", \"Word Embedding\", \"Explainable Artificial Intelligence (XAI)\",\n",
    "            \"Language Model\", \"Pre-Trained Language Model\", \"Multimodal Learning\", \"Image Classification\",\n",
    "            \"Machine Translation\", \"Natural Language Understanding (NLU)\", \"Text Generation\",\n",
    "            \"Meta Learning\", \"Continual Learning\", \"Mixture of Experts\", \"Ensemble\",\n",
    "            \"Model Compression\", \"Knoweldge Distillation\", \"Quantization\", \"Pruning\",\n",
    "            \"Low-Rank Factorization\", \"Early Exit\", \"Reinforcement Learning\", \"Security\", \"Computer Vision\"]\n",
    "\n",
    "arxiv_paper_df_with_abstract_by_theme = make_arxiv_paper_df_with_abstract_by_theme(theme_order, arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df = make_arxiv_paper_df(arxiv_paper_df_with_abstract)\n",
    "arxiv_paper_df_by_theme = make_arxiv_paper_df_by_theme(arxiv_paper_df_with_abstract_by_theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Adaptive Mixtures of Local Experts</td>\n",
       "      <td>MIT Press 1991</td>\n",
       "      <td>1991-03-01</td>\n",
       "      <td>Robert A. Jacobs et al</td>\n",
       "      <td>Mixture of Experts</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>We present a new supervised learning procedure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BLEU: a method for automatic evaluation of mac...</td>\n",
       "      <td>ACL 2002</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>Kishore Papineni et al</td>\n",
       "      <td>Benchmark</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/10.3115/1073083....</td>\n",
       "      <td>Human evaluations of machine translation are e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Model Compression</td>\n",
       "      <td>ACM SIGKDD 2006</td>\n",
       "      <td>2006-08-20</td>\n",
       "      <td>Cristian Bucil˘a et al</td>\n",
       "      <td>Model Compression</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/abs/10.1145/1150...</td>\n",
       "      <td>Often the best performing supervised learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Large-Scale Distributed Language Modeling</td>\n",
       "      <td>IEEE 2007</td>\n",
       "      <td>2007-04-05</td>\n",
       "      <td>Ahmad Emami et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/document/42...</td>\n",
       "      <td>A novel distributed language model that has no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Large Language Models in Machine Translation</td>\n",
       "      <td>Teaching and Learning in Higher Education</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>Gloria Brown Wright et al</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>[Link](https://aclanthology.org/D07-1090/)</td>\n",
       "      <td>This paper reports on the benefits of largesca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>133</td>\n",
       "      <td>QLoRA: Efficient Finetuning of Quantized LLMs</td>\n",
       "      <td>EMNLP 2023</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Tim Dettmers et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2305.14314v1)</td>\n",
       "      <td>We present QLoRA, an efficient finetuning appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>134</td>\n",
       "      <td>Contrastive Learning of Sentence Embeddings fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>Junlei Zhang et al</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2305.15077v2)</td>\n",
       "      <td>Contrastive learning has been the dominant app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>135</td>\n",
       "      <td>An Empirical Comparison of LM-based Question a...</td>\n",
       "      <td>ACL  2023</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>Asahi Ushio et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2305.17002v1)</td>\n",
       "      <td>Question and answer generation (QAG) consists ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>136</td>\n",
       "      <td>A Practical Toolkit for Multilingual Question ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>Asahi Ushio et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2305.17416v1)</td>\n",
       "      <td>Generating questions along with associated ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>137</td>\n",
       "      <td>LLM-QAT: Data-Free Quantization Aware Training...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29</td>\n",
       "      <td>Zechun Liu et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2305.17888v1)</td>\n",
       "      <td>Several post-training quantization methods hav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                              Title  \\\n",
       "0             1                 Adaptive Mixtures of Local Experts   \n",
       "1             2  BLEU: a method for automatic evaluation of mac...   \n",
       "2             3                                  Model Compression   \n",
       "3             4          Large-Scale Distributed Language Modeling   \n",
       "4             5       Large Language Models in Machine Translation   \n",
       "..          ...                                                ...   \n",
       "132         133      QLoRA: Efficient Finetuning of Quantized LLMs   \n",
       "133         134  Contrastive Learning of Sentence Embeddings fr...   \n",
       "134         135  An Empirical Comparison of LM-based Question a...   \n",
       "135         136  A Practical Toolkit for Multilingual Question ...   \n",
       "136         137  LLM-QAT: Data-Free Quantization Aware Training...   \n",
       "\n",
       "                            Journal/Conference       Date  \\\n",
       "0                               MIT Press 1991 1991-03-01   \n",
       "1                                     ACL 2002 2002-07-01   \n",
       "2                              ACM SIGKDD 2006 2006-08-20   \n",
       "3                                    IEEE 2007 2007-04-05   \n",
       "4    Teaching and Learning in Higher Education 2007-06-01   \n",
       "..                                         ...        ...   \n",
       "132                                 EMNLP 2023 2023-05-23   \n",
       "133                                        NaN 2023-05-24   \n",
       "134                                  ACL  2023 2023-05-26   \n",
       "135                                        NaN 2023-05-27   \n",
       "136                                        NaN 2023-05-29   \n",
       "\n",
       "                        Author                Theme  \\\n",
       "0       Robert A. Jacobs et al   Mixture of Experts   \n",
       "1       Kishore Papineni et al            Benchmark   \n",
       "2       Cristian Bucil˘a et al    Model Compression   \n",
       "3            Ahmad Emami et al       Language Model   \n",
       "4    Gloria Brown Wright et al  Machine Translation   \n",
       "..                         ...                  ...   \n",
       "132         Tim Dettmers et al         Quantization   \n",
       "133         Junlei Zhang et al       Word Embedding   \n",
       "134          Asahi Ushio et al       Language Model   \n",
       "135          Asahi Ushio et al       Language Model   \n",
       "136           Zechun Liu et al         Quantization   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    [Link](https://ieeexplore.ieee.org/abstract/do...   \n",
       "1    [Link](https://dl.acm.org/doi/10.3115/1073083....   \n",
       "2    [Link](https://dl.acm.org/doi/abs/10.1145/1150...   \n",
       "3    [Link](https://ieeexplore.ieee.org/document/42...   \n",
       "4           [Link](https://aclanthology.org/D07-1090/)   \n",
       "..                                                 ...   \n",
       "132          [Link](http://arxiv.org/abs/2305.14314v1)   \n",
       "133          [Link](http://arxiv.org/abs/2305.15077v2)   \n",
       "134          [Link](http://arxiv.org/abs/2305.17002v1)   \n",
       "135          [Link](http://arxiv.org/abs/2305.17416v1)   \n",
       "136          [Link](http://arxiv.org/abs/2305.17888v1)   \n",
       "\n",
       "                                              Abstract  \n",
       "0    We present a new supervised learning procedure...  \n",
       "1    Human evaluations of machine translation are e...  \n",
       "2    Often the best performing supervised learning ...  \n",
       "3    A novel distributed language model that has no...  \n",
       "4    This paper reports on the benefits of largesca...  \n",
       "..                                                 ...  \n",
       "132  We present QLoRA, an efficient finetuning appr...  \n",
       "133  Contrastive learning has been the dominant app...  \n",
       "134  Question and answer generation (QAG) consists ...  \n",
       "135  Generating questions along with associated ans...  \n",
       "136  Several post-training quantization methods hav...  \n",
       "\n",
       "[137 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Theme</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Neural Network</th>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>Dropout: A Simple Way to Prevent Neural Networ...</td>\n",
       "      <td>JMLR 2014</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>Nitish Srivastava et al</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>Deep neural nets with a large number of parame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Convolutional Neural Networks for Sentence Cla...</td>\n",
       "      <td>EMNLP  2014</td>\n",
       "      <td>2014-08-25</td>\n",
       "      <td>Yoon Kim et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1408.5882v2)</td>\n",
       "      <td>We report on a series of experiments with conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Convolutional Neural Networks using Logarithmi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-03</td>\n",
       "      <td>Daisuke Miyashita et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1603.01025v2)</td>\n",
       "      <td>Recent advances in convolutional neural networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>Permutation Invariant Training of Deep Models ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>Dong Yu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1607.00325v2)</td>\n",
       "      <td>We propose a novel deep learning model, which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>Accurate, Large Minibatch SGD: Training ImageN...</td>\n",
       "      <td>NeurIPS 2017</td>\n",
       "      <td>2017-06-08</td>\n",
       "      <td>Priya Goyal et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1706.02677v2)</td>\n",
       "      <td>Deep learning thrives with large neural networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Computer Vision</th>\n",
       "      <th>133</th>\n",
       "      <td>41</td>\n",
       "      <td>The Perception-Distortion Tradeoff</td>\n",
       "      <td>CVPR  2018</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>Yochai Blau et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1711.06077v4)</td>\n",
       "      <td>Image restoration algorithms are typically eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>50</td>\n",
       "      <td>Do CIFAR-10 Classifiers Generalize to CIFAR-10?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>Benjamin Recht et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1806.00451v1)</td>\n",
       "      <td>Machine learning is currently dominated by lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>54</td>\n",
       "      <td>Making Sense of Vision and Touch: Self-Supervi...</td>\n",
       "      <td>ICRA  2019</td>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>Michelle A. Lee et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1810.10191v2)</td>\n",
       "      <td>Contact-rich manipulation tasks in unstructure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>92</td>\n",
       "      <td>An Image is Worth 16x16 Words: Transformers fo...</td>\n",
       "      <td>CVPR 2021</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>Alexey Dosovitskiy et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2010.11929v2)</td>\n",
       "      <td>While the Transformer architecture has become ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>94</td>\n",
       "      <td>Training data-efficient image transformers &amp; d...</td>\n",
       "      <td>ACL 2021</td>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>Hugo Touvron et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2012.12877v2)</td>\n",
       "      <td>Recently, neural networks purely based on atte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0  \\\n",
       "Theme                             \n",
       "Neural Network  1            11   \n",
       "                2            12   \n",
       "                3            20   \n",
       "                4            26   \n",
       "                5            33   \n",
       "...                         ...   \n",
       "Computer Vision 133          41   \n",
       "                134          50   \n",
       "                135          54   \n",
       "                136          92   \n",
       "                137          94   \n",
       "\n",
       "                                                                 Title  \\\n",
       "Theme                                                                    \n",
       "Neural Network  1    Dropout: A Simple Way to Prevent Neural Networ...   \n",
       "                2    Convolutional Neural Networks for Sentence Cla...   \n",
       "                3    Convolutional Neural Networks using Logarithmi...   \n",
       "                4    Permutation Invariant Training of Deep Models ...   \n",
       "                5    Accurate, Large Minibatch SGD: Training ImageN...   \n",
       "...                                                                ...   \n",
       "Computer Vision 133                 The Perception-Distortion Tradeoff   \n",
       "                134    Do CIFAR-10 Classifiers Generalize to CIFAR-10?   \n",
       "                135  Making Sense of Vision and Touch: Self-Supervi...   \n",
       "                136  An Image is Worth 16x16 Words: Transformers fo...   \n",
       "                137  Training data-efficient image transformers & d...   \n",
       "\n",
       "                    Journal/Conference       Date                    Author  \\\n",
       "Theme                                                                         \n",
       "Neural Network  1            JMLR 2014 2014-01-01   Nitish Srivastava et al   \n",
       "                2          EMNLP  2014 2014-08-25            Yoon Kim et al   \n",
       "                3                  NaN 2016-03-03   Daisuke Miyashita et al   \n",
       "                4                  NaN 2016-07-01             Dong Yu et al   \n",
       "                5         NeurIPS 2017 2017-06-08         Priya Goyal et al   \n",
       "...                                ...        ...                       ...   \n",
       "Computer Vision 133         CVPR  2018 2017-11-16         Yochai Blau et al   \n",
       "                134                NaN 2018-06-01      Benjamin Recht et al   \n",
       "                135         ICRA  2019 2018-10-24     Michelle A. Lee et al   \n",
       "                136          CVPR 2021 2020-10-22  Alexey Dosovitskiy et al   \n",
       "                137           ACL 2021 2020-12-23        Hugo Touvron et al   \n",
       "\n",
       "                                                                  Link  \\\n",
       "Theme                                                                    \n",
       "Neural Network  1    [Link](https://ieeexplore.ieee.org/abstract/do...   \n",
       "                2             [Link](http://arxiv.org/abs/1408.5882v2)   \n",
       "                3            [Link](http://arxiv.org/abs/1603.01025v2)   \n",
       "                4            [Link](http://arxiv.org/abs/1607.00325v2)   \n",
       "                5            [Link](http://arxiv.org/abs/1706.02677v2)   \n",
       "...                                                                ...   \n",
       "Computer Vision 133          [Link](http://arxiv.org/abs/1711.06077v4)   \n",
       "                134          [Link](http://arxiv.org/abs/1806.00451v1)   \n",
       "                135          [Link](http://arxiv.org/abs/1810.10191v2)   \n",
       "                136          [Link](http://arxiv.org/abs/2010.11929v2)   \n",
       "                137          [Link](http://arxiv.org/abs/2012.12877v2)   \n",
       "\n",
       "                                                              Abstract  \n",
       "Theme                                                                   \n",
       "Neural Network  1    Deep neural nets with a large number of parame...  \n",
       "                2    We report on a series of experiments with conv...  \n",
       "                3    Recent advances in convolutional neural networ...  \n",
       "                4    We propose a novel deep learning model, which ...  \n",
       "                5    Deep learning thrives with large neural networ...  \n",
       "...                                                                ...  \n",
       "Computer Vision 133  Image restoration algorithms are typically eva...  \n",
       "                134  Machine learning is currently dominated by lar...  \n",
       "                135  Contact-rich manipulation tasks in unstructure...  \n",
       "                136  While the Transformer architecture has become ...  \n",
       "                137  Recently, neural networks purely based on atte...  \n",
       "\n",
       "[137 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract_by_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract.to_excel(\"arxiv_paper_df_with_abstract.xlsx\")\n",
    "arxiv_paper_df.to_excel(\"arxiv_paper_df.xlsx\")\n",
    "# arxiv_paper_df_with_abstract = pd.read_excel(\"arxiv_paper_df_with_abstract.xlsx\", engine='openpyxl')\n",
    "\n",
    "arxiv_paper_df_with_abstract_by_theme.to_excel(\"arxiv_paper_df_with_abstract_by_theme.xlsx\")\n",
    "arxiv_paper_df_by_theme.to_excel(\"arxiv_paper_df_by_theme.xlsx\")\n",
    "# arxiv_paper_df_with_abstract_by_theme = pd.read_excel(\"arxiv_paper_df_with_abstract_by_theme.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataframe on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Excel to Markdown Converter](https://tabletomarkdown.com/convert-spreadsheet-to-markdown/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
