{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Paper Dataframe by Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install arxiv\n",
    "%pip install clipboard\n",
    "%pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import arxiv\n",
    "import clipboard\n",
    "import pyautogui\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_with_abstract(paper_ids):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame({'Title':['Noun'],\n",
    "                               'Journal/Conference':['Noun'],\n",
    "                               'Date':['Noun'], \n",
    "                               'Author':['Noun'],\n",
    "                               'Link':['Noun'],\n",
    "                               'Abstract':['Noun']})\n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    pbar = tqdm.tqdm(paper_ids)\n",
    "\n",
    "    for idx, paper_id in enumerate(pbar):\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(client.results(search))\n",
    "        \n",
    "        paper_journal_conf = re.search(r'[A-Z ]+[0-9]+[0-9]+[0-9]+[0-9]', str(paper.comment))\n",
    "        if paper_journal_conf != None:\n",
    "            paper_journal_conf = paper_journal_conf.group().strip()\n",
    "            if len(paper_journal_conf) > 4:\n",
    "                if paper_journal_conf[-4] != \" \":\n",
    "                    paper_journal_conf = paper_journal_conf[:-4] + \" \" + paper_journal_conf[-4:]\n",
    "                else:\n",
    "                    paper_journal_conf = paper_journal_conf\n",
    "            elif len(paper_journal_conf) <= 4:\n",
    "                paper_journal_conf = \"\"\n",
    "        elif paper_journal_conf == None:\n",
    "            paper_journal_conf = \"\"\n",
    "\n",
    "        arxiv_paper_df_with_abstract.loc[idx] = [paper.title, \n",
    "                                paper_journal_conf,\n",
    "                                paper.published.date(), \n",
    "                                str(paper.authors[0]) + ' et al',\n",
    "                                    paper.entry_id,\n",
    "                                    paper.summary]\n",
    "    pbar.close()\n",
    "    \n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)         \n",
    "    \n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_convert_datetime(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_other_papers_column(arxiv_paper_df_with_abstract, other_papers):\n",
    "  \n",
    "  df_length = len(arxiv_paper_df_with_abstract) - 1\n",
    "\n",
    "  pbar = tqdm.tqdm(other_papers)\n",
    "\n",
    "  for other_paper in pbar:\n",
    "    df_length += 1\n",
    "    arxiv_paper_df_with_abstract.loc[df_length] = other_paper\n",
    "  \n",
    "  arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "  arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)  \n",
    "  \n",
    "  pbar.close()\n",
    "  \n",
    "  return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperlink(x):\n",
    "    hyperlink= '[Link]' + '(' + x + ')'\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_jouranl_conference_theme(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    paper_title = arxiv_paper_df_with_abstract['Title']\n",
    "    paper_journal_conference = arxiv_paper_df_with_abstract['Journal/Conference']\n",
    "    arxiv_paper_df_with_abstract['Theme'] = \"\"\n",
    "    paper_theme = arxiv_paper_df_with_abstract['Theme']\n",
    "\n",
    "    pyautogui.alert('Input Paper Jouranl Conference')\n",
    "\n",
    "    for index, (title, journal_conference) in enumerate(zip(paper_title, paper_journal_conference)):\n",
    "\n",
    "        if len(journal_conference) < 2:\n",
    "            clipboard.copy(title)\n",
    "            input_journal_conference = input(\"{} For {}: \".format(\"Input Journal & Conference\", title)) \n",
    "            arxiv_paper_df_with_abstract.loc[index, 'Journal/Conference'] = input_journal_conference\n",
    "\n",
    "    pyautogui.alert('Input Paper Theme')\n",
    "\n",
    "    for index, (title, theme) in enumerate(zip(paper_title, paper_theme)):\n",
    "\n",
    "        if len(theme) < 2:\n",
    "            clipboard.copy(title)\n",
    "            input_theme = input(\"{} For {}: \".format(\"Input Theme\", title)) \n",
    "            arxiv_paper_df_with_abstract.loc[index, 'Theme'] = input_theme\n",
    "\n",
    "    arxiv_paper_df_with_abstract = arxiv_paper_df_with_abstract[['Title', 'Journal/Conference', 'Date', 'Author', 'Theme', 'Link', 'Abstract']]\n",
    "\n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_with_abstract_by_theme(theme_order, arxiv_paper_df_with_abstract):\n",
    "\n",
    "    def sorter(column):\n",
    "        mapper = {name: order for order, name in enumerate(theme_order)}\n",
    "        return column.map(mapper)\n",
    "\n",
    "    arxiv_paper_df_with_abstract_by_theme = arxiv_paper_df_with_abstract.sort_values(by=['Theme', 'Date'], key=sorter, ascending=True).reset_index() \n",
    "    del arxiv_paper_df_with_abstract_by_theme['index']\n",
    "    arxiv_paper_df_with_abstract_by_theme.index += 1 \n",
    "    arxiv_paper_df_with_abstract_by_theme = arxiv_paper_df_with_abstract_by_theme.set_index('Theme', append=True).swaplevel(0, 1)\n",
    "\n",
    "    return arxiv_paper_df_with_abstract_by_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)    \n",
    "    arxiv_paper_df = arxiv_paper_df_with_abstract.drop(['Abstract'], axis='columns')\n",
    "\n",
    "    return arxiv_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_by_theme(arxiv_paper_df_with_abstract_by_theme):\n",
    "  \n",
    "    arxiv_paper_df_by_theme = arxiv_paper_df_with_abstract_by_theme.drop(['Abstract'], axis='columns')\n",
    "\n",
    "    return arxiv_paper_df_by_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [04:12<00:00,  3.56s/it]\n"
     ]
    }
   ],
   "source": [
    "paper_ids = [\"2403.09032\", \"2403.08295\", \"1911.02150\", \"2104.09864\",\n",
    "             \"2205.01543\", \"1908.10084\", \"2310.06825\", \"2110.05679\",\n",
    "             \"1610.05820\", \"2012.07805\", \"1611.03530\", \"2305.04388\",\n",
    "             \"2010.06053\", \"1607.00133\", \"2009.03106\", \"2011.11660\",\n",
    "             \"1510.01799\", \"2206.11309\", \"1801.07243\", \"1706.09254\",\n",
    "             \"1412.6980\", \"1908.08345\", \"2210.03992\", \"2105.09680\",\n",
    "             \"2106.14448\", \"2307.13304\", \"2205.03835\", \"1704.04368\",\n",
    "             \"1611.04230\", \"2205.11315\", \"2004.12832\", \"2212.09114\",\n",
    "             \"2212.09114\", \"1906.00300\", \"2004.04906\", \"2012.12624\", \n",
    "             \"1704.00051\", \"2208.04232\", \"2104.08663\", \"2203.05794\",\n",
    "             \"1904.08375\", \"2203.08372\", \"1805.04833\", \"2108.05540\",\n",
    "             \"2104.00369\", \"1904.09675\", \"2004.04696\", \"1804.08771\",\n",
    "             \"2304.11015\", \"2106.15339\", \"1901.11196\", \"2003.02245\",\n",
    "             \"2004.12239\", \"1904.09545\", \"2105.07624\", \"2305.02301\",\n",
    "             \"1711.09846\", \"1908.07442\", \"2012.06678\", \"2207.08815\",\n",
    "             \"1603.02754\", \"2301.13808\", \"2004.02349\", \"2112.07337\",\n",
    "             \"2205.14690\", \"2207.03637\", \"1608.03983\", \"1508.00305\",\n",
    "             \"2004.14373\", \"1709.00103\", \"1511.06335\"]\n",
    "\n",
    "arxiv_paper_df_with_abstract = make_arxiv_paper_df_with_abstract(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 854.38it/s]\n"
     ]
    }
   ],
   "source": [
    "other_papers = [[\"A Recurrent BERT-based Model for Question Generation\", \"ACL 2019\", str_convert_datetime(\"2019-01-01\"),\n",
    "                \"Ying-Hong Chan et al\", \"https://aclanthology.org/D19-5821/\", \n",
    "                \"In this study, we investigate the employment of the pre-trained BERT language model to tackle question generation tasks. We introduce three neural architectures built on top of BERT for question generation tasks. The first one is a straightforward BERT employment, which reveals the defects of directly using BERT for text generation. Accordingly, we propose another two models by restructuring our BERT employment into a sequential manner for taking information from previous decoded results. Our models are trained and evaluated on the recent question-answering dataset SQuAD. Experiment results show that our best model yields state-of-the-art performance which advances the BLEU 4 score of the existing best models from 16.85 to 22.17.\"],\n",
    "                [\"Hierarchical Attention Networks for Document Classification\", \"NAACL 2016\", str_convert_datetime(\"2016-01-01\"),\n",
    "                \"Zichao Yang et al\", \"https://aclanthology.org/N16-1174/\", \n",
    "                \"We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\"], \n",
    "                [\"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments\", \"ACL 2005\", str_convert_datetime(\"2005-01-01\"),\n",
    "                \"Satanjeev Banerjee et al\", \"https://aclanthology.org/W05-0909/9\", \n",
    "                \"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.\"],\n",
    "                [\"Random Forests\", \"Machine Learning, Volume 45\", str_convert_datetime(\"2001-01-01\"),\n",
    "                \"Leo Breiman\", \"https://link.springer.com/article/10.1023/A:1010933404324\", \n",
    "                \"Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.\"],\n",
    "                 [\"DoT: An efficient Double Transformer for NLP tasks with tables\", \"ACL Findings 2021\", str_convert_datetime(\"2021-01-01\"),\n",
    "                \"Syrine Krichene et al\", \"https://aclanthology.org/2021.findings-acl.289/\", \n",
    "                \"Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model.\"],\n",
    "                 [\"Visualizing Data using t-SNE\", \"JMLR 2008\", str_convert_datetime(\"2008-01-01\"),\n",
    "                \"Laurens van der Maaten et al\", \"https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\", \n",
    "                \"We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.\"] ]\n",
    "\\\n",
    "arxiv_paper_df_with_abstract = add_other_papers_column(arxiv_paper_df_with_abstract, other_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract[\"Link\"] = arxiv_paper_df_with_abstract[\"Link\"].apply(hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract = input_jouranl_conference_theme(arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract.to_excel('arxiv_paper_df_with_abstract.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract = pd.read_excel('arxiv_paper_df_with_abstract.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_order = [\"Language Model\", \"Security\", \"Benchmark\", \n",
    "               \"Neural Network\", \"Information Retrieval\", \n",
    "               \"Tabular Learning\", \"Knowledge Distillation\"]\n",
    "\n",
    "arxiv_paper_df_with_abstract_by_theme = make_arxiv_paper_df_with_abstract_by_theme(theme_order, arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>Machine Learning, Volume 45</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Leo Breiman</td>\n",
       "      <td>Benchmark</td>\n",
       "      <td>[Link](https://link.springer.com/article/10.10...</td>\n",
       "      <td>Random forests are a combination of tree predi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METEOR: An Automatic Metric for MT Evaluation ...</td>\n",
       "      <td>ACL 2005</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Satanjeev Banerjee et al</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>[Link](https://aclanthology.org/W05-0909/9)</td>\n",
       "      <td>We describe METEOR, an automatic metric for ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visualizing Data using t-SNE</td>\n",
       "      <td>ICLR 2015</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>Laurens van der Maaten et al</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>[Link](https://www.jmlr.org/papers/volume9/van...</td>\n",
       "      <td>We present a new technique called “t-SNE” that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adam: A Method for Stochastic Optimization</td>\n",
       "      <td>ACL 2015</td>\n",
       "      <td>2014-12-22</td>\n",
       "      <td>Diederik P. Kingma et al</td>\n",
       "      <td>Tabular Learning</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1412.6980v9)</td>\n",
       "      <td>We introduce Adam, an algorithm for first-orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compositional Semantic Parsing on Semi-Structu...</td>\n",
       "      <td></td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>Panupong Pasupat et al</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1508.00305v1)</td>\n",
       "      <td>Two important aspects of semantic parsing for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Distilling Step-by-Step! Outperforming Larger ...</td>\n",
       "      <td>ACL  2023</td>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>Cheng-Yu Hsieh et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2305.02301v2)</td>\n",
       "      <td>Deploying large language models (LLMs) is chal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Language Models Don't Always Say What They Thi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>Miles Turpin et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2305.04388v2)</td>\n",
       "      <td>Large Language Models (LLMs) can achieve stron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>QuIP: 2-Bit Quantization of Large Language Mod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>Jerry Chee et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2307.13304v2)</td>\n",
       "      <td>This work studies post-training parameter quan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-10</td>\n",
       "      <td>Albert Q. Jiang et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2310.06825v1)</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Gemma: Open Models Based on Gemini Research an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-03-13</td>\n",
       "      <td>Gemma Team et al</td>\n",
       "      <td>Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2403.08295v4)</td>\n",
       "      <td>This work introduces Gemma, a family of lightw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                      Random Forests   \n",
       "1   METEOR: An Automatic Metric for MT Evaluation ...   \n",
       "2                        Visualizing Data using t-SNE   \n",
       "3          Adam: A Method for Stochastic Optimization   \n",
       "4   Compositional Semantic Parsing on Semi-Structu...   \n",
       "..                                                ...   \n",
       "71  Distilling Step-by-Step! Outperforming Larger ...   \n",
       "72  Language Models Don't Always Say What They Thi...   \n",
       "73  QuIP: 2-Bit Quantization of Large Language Mod...   \n",
       "74                                         Mistral 7B   \n",
       "75  Gemma: Open Models Based on Gemini Research an...   \n",
       "\n",
       "             Journal/Conference       Date                        Author  \\\n",
       "0   Machine Learning, Volume 45 2001-01-01                   Leo Breiman   \n",
       "1                      ACL 2005 2005-01-01      Satanjeev Banerjee et al   \n",
       "2                     ICLR 2015 2008-01-01  Laurens van der Maaten et al   \n",
       "3                      ACL 2015 2014-12-22      Diederik P. Kingma et al   \n",
       "4                               2015-08-03        Panupong Pasupat et al   \n",
       "..                          ...        ...                           ...   \n",
       "71                    ACL  2023 2023-05-03          Cheng-Yu Hsieh et al   \n",
       "72                          NaN 2023-05-07            Miles Turpin et al   \n",
       "73                          NaN 2023-07-25              Jerry Chee et al   \n",
       "74                          NaN 2023-10-10         Albert Q. Jiang et al   \n",
       "75                          NaN 2024-03-13              Gemma Team et al   \n",
       "\n",
       "               Theme                                               Link  \\\n",
       "0          Benchmark  [Link](https://link.springer.com/article/10.10...   \n",
       "1     Neural Network        [Link](https://aclanthology.org/W05-0909/9)   \n",
       "2     Neural Network  [Link](https://www.jmlr.org/papers/volume9/van...   \n",
       "3   Tabular Learning           [Link](http://arxiv.org/abs/1412.6980v9)   \n",
       "4     Neural Network          [Link](http://arxiv.org/abs/1508.00305v1)   \n",
       "..               ...                                                ...   \n",
       "71    Language Model          [Link](http://arxiv.org/abs/2305.02301v2)   \n",
       "72    Language Model          [Link](http://arxiv.org/abs/2305.04388v2)   \n",
       "73    Language Model          [Link](http://arxiv.org/abs/2307.13304v2)   \n",
       "74    Language Model          [Link](http://arxiv.org/abs/2310.06825v1)   \n",
       "75    Language Model          [Link](http://arxiv.org/abs/2403.08295v4)   \n",
       "\n",
       "                                             Abstract  \n",
       "0   Random forests are a combination of tree predi...  \n",
       "1   We describe METEOR, an automatic metric for ma...  \n",
       "2   We present a new technique called “t-SNE” that...  \n",
       "3   We introduce Adam, an algorithm for first-orde...  \n",
       "4   Two important aspects of semantic parsing for ...  \n",
       "..                                                ...  \n",
       "71  Deploying large language models (LLMs) is chal...  \n",
       "72  Large Language Models (LLMs) can achieve stron...  \n",
       "73  This work studies post-training parameter quan...  \n",
       "74  We introduce Mistral 7B v0.1, a 7-billion-para...  \n",
       "75  This work introduces Gemma, a family of lightw...  \n",
       "\n",
       "[76 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Theme</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Language Model</th>\n",
       "      <th>1</th>\n",
       "      <td>Unsupervised Deep Embedding for Clustering Ana...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>Junyuan Xie et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1511.06335v2)</td>\n",
       "      <td>Clustering is central to many data-driven appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Understanding deep learning requires rethinkin...</td>\n",
       "      <td>ICLR  2017</td>\n",
       "      <td>2016-11-10</td>\n",
       "      <td>Chiyuan Zhang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1611.03530v2)</td>\n",
       "      <td>Despite their massive size, successful deep ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reading Wikipedia to Answer Open-Domain Questions</td>\n",
       "      <td>ACL 2017</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>Danqi Chen et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1704.00051v2)</td>\n",
       "      <td>This paper proposes to tackle open- domain que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Population Based Training of Neural Networks</td>\n",
       "      <td>ACL 2018</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>Max Jaderberg et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1711.09846v2)</td>\n",
       "      <td>Neural networks dominate the modern machine le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Call for Clarity in Reporting BLEU Scores</td>\n",
       "      <td>ACL 2018</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>Matt Post et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1804.08771v2)</td>\n",
       "      <td>The field of machine translation faces an unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Tabular Learning</th>\n",
       "      <th>72</th>\n",
       "      <td>OmniTab: Pretraining with Natural and Syntheti...</td>\n",
       "      <td>NeurIPS 2022</td>\n",
       "      <td>2022-07-08</td>\n",
       "      <td>Zhengbao Jiang et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2207.03637v1)</td>\n",
       "      <td>The information in tables can be an important ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>CAPSTONE: Curriculum Sampling for Dense Retrie...</td>\n",
       "      <td>EMNLP  2023</td>\n",
       "      <td>2022-12-18</td>\n",
       "      <td>Xingwei He et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2212.09114v2)</td>\n",
       "      <td>The dual-encoder has become the de facto archi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Large Language Models are Versatile Decomposer...</td>\n",
       "      <td>SIGIR  2023</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>Yunhu Ye et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2301.13808v3)</td>\n",
       "      <td>Table-based reasoning has shown remarkable pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Knowledge Distillation</th>\n",
       "      <th>75</th>\n",
       "      <td>DIN-SQL: Decomposed In-Context Learning of Tex...</td>\n",
       "      <td>IPS  2023</td>\n",
       "      <td>2023-04-21</td>\n",
       "      <td>Mohammadreza Pourreza et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2304.11015v3)</td>\n",
       "      <td>There is currently a significant gap between t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language Model</th>\n",
       "      <th>76</th>\n",
       "      <td>Text Summarization with Pretrained Encoders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>Yang Liu et al</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1908.08345v2)</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Title  \\\n",
       "Theme                                                                          \n",
       "Language Model         1   Unsupervised Deep Embedding for Clustering Ana...   \n",
       "                       2   Understanding deep learning requires rethinkin...   \n",
       "                       3   Reading Wikipedia to Answer Open-Domain Questions   \n",
       "                       4        Population Based Training of Neural Networks   \n",
       "                       5         A Call for Clarity in Reporting BLEU Scores   \n",
       "...                                                                      ...   \n",
       "Tabular Learning       72  OmniTab: Pretraining with Natural and Syntheti...   \n",
       "                       73  CAPSTONE: Curriculum Sampling for Dense Retrie...   \n",
       "                       74  Large Language Models are Versatile Decomposer...   \n",
       "Knowledge Distillation 75  DIN-SQL: Decomposed In-Context Learning of Tex...   \n",
       " Language Model        76        Text Summarization with Pretrained Encoders   \n",
       "\n",
       "                          Journal/Conference       Date  \\\n",
       "Theme                                                     \n",
       "Language Model         1                 NaN 2015-11-19   \n",
       "                       2          ICLR  2017 2016-11-10   \n",
       "                       3            ACL 2017 2017-03-31   \n",
       "                       4            ACL 2018 2017-11-27   \n",
       "                       5            ACL 2018 2018-04-23   \n",
       "...                                      ...        ...   \n",
       "Tabular Learning       72       NeurIPS 2022 2022-07-08   \n",
       "                       73        EMNLP  2023 2022-12-18   \n",
       "                       74        SIGIR  2023 2023-01-31   \n",
       "Knowledge Distillation 75          IPS  2023 2023-04-21   \n",
       " Language Model        76                NaN 2019-08-22   \n",
       "\n",
       "                                                Author  \\\n",
       "Theme                                                    \n",
       "Language Model         1             Junyuan Xie et al   \n",
       "                       2           Chiyuan Zhang et al   \n",
       "                       3              Danqi Chen et al   \n",
       "                       4           Max Jaderberg et al   \n",
       "                       5               Matt Post et al   \n",
       "...                                                ...   \n",
       "Tabular Learning       72         Zhengbao Jiang et al   \n",
       "                       73             Xingwei He et al   \n",
       "                       74               Yunhu Ye et al   \n",
       "Knowledge Distillation 75  Mohammadreza Pourreza et al   \n",
       " Language Model        76               Yang Liu et al   \n",
       "\n",
       "                                                                Link  \\\n",
       "Theme                                                                  \n",
       "Language Model         1   [Link](http://arxiv.org/abs/1511.06335v2)   \n",
       "                       2   [Link](http://arxiv.org/abs/1611.03530v2)   \n",
       "                       3   [Link](http://arxiv.org/abs/1704.00051v2)   \n",
       "                       4   [Link](http://arxiv.org/abs/1711.09846v2)   \n",
       "                       5   [Link](http://arxiv.org/abs/1804.08771v2)   \n",
       "...                                                              ...   \n",
       "Tabular Learning       72  [Link](http://arxiv.org/abs/2207.03637v1)   \n",
       "                       73  [Link](http://arxiv.org/abs/2212.09114v2)   \n",
       "                       74  [Link](http://arxiv.org/abs/2301.13808v3)   \n",
       "Knowledge Distillation 75  [Link](http://arxiv.org/abs/2304.11015v3)   \n",
       " Language Model        76  [Link](http://arxiv.org/abs/1908.08345v2)   \n",
       "\n",
       "                                                                    Abstract  \n",
       "Theme                                                                         \n",
       "Language Model         1   Clustering is central to many data-driven appl...  \n",
       "                       2   Despite their massive size, successful deep ar...  \n",
       "                       3   This paper proposes to tackle open- domain que...  \n",
       "                       4   Neural networks dominate the modern machine le...  \n",
       "                       5   The field of machine translation faces an unde...  \n",
       "...                                                                      ...  \n",
       "Tabular Learning       72  The information in tables can be an important ...  \n",
       "                       73  The dual-encoder has become the de facto archi...  \n",
       "                       74  Table-based reasoning has shown remarkable pro...  \n",
       "Knowledge Distillation 75  There is currently a significant gap between t...  \n",
       " Language Model        76  Bidirectional Encoder Representations from Tra...  \n",
       "\n",
       "[76 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract_by_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract.to_excel(\"arxiv_paper_df_with_abstract.xlsx\")\n",
    "arxiv_paper_df.to_excel(\"arxiv_paper_df.xlsx\")\n",
    "# arxiv_paper_df_with_abstract = pd.read_excel(\"arxiv_paper_df_with_abstract.xlsx\", engine='openpyxl')\n",
    "\n",
    "arxiv_paper_df_with_abstract_by_theme.to_excel(\"arxiv_paper_df_with_abstract_by_theme.xlsx\")\n",
    "arxiv_paper_df_by_theme.to_excel(\"arxiv_paper_df_by_theme.xlsx\")\n",
    "# arxiv_paper_df_with_abstract_by_theme = pd.read_excel(\"arxiv_paper_df_with_abstract_by_theme.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataframe on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Excel to Markdown Converter](https://tabletomarkdown.com/convert-spreadsheet-to-markdown/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
