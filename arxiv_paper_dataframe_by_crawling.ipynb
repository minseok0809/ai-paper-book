{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Paper Dataframe by Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install clipboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import arxiv\n",
    "import clipboard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df_with_abstract(paper_ids):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame({'Title':[0],\n",
    "                               'Journal/Conference':[0],\n",
    "                               'Date':[0], \n",
    "                               'Author':[0],\n",
    "                               'Link':[0],\n",
    "                               'Abstract':[0]})\n",
    "    \n",
    "    for idx, paper_id in enumerate(paper_ids):\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(search.results())\n",
    "        \n",
    "        paper_journal_conf = re.search(r'[A-Z ]+[0-9]+[0-9]+[0-9]+[0-9]', str(paper.comment))\n",
    "        if paper_journal_conf != None:\n",
    "            paper_journal_conf = paper_journal_conf.group().strip()\n",
    "            if len(paper_journal_conf) > 4:\n",
    "                if paper_journal_conf[-4] != \" \":\n",
    "                    paper_journal_conf = paper_journal_conf[:-4] + \" \" + paper_journal_conf[-4:]\n",
    "                else:\n",
    "                    paper_journal_conf = paper_journal_conf\n",
    "            elif len(paper_journal_conf) <= 4:\n",
    "                paper_journal_conf = \"\"\n",
    "        elif paper_journal_conf == None:\n",
    "            paper_journal_conf = \"\"\n",
    "\n",
    "        arxiv_paper_df_with_abstract.loc[idx] = [paper.title, \n",
    "                                paper_journal_conf,\n",
    "                                paper.published.date(), \n",
    "                                str(paper.authors[0]) + ' et al',\n",
    "                                    paper.entry_id,\n",
    "                                    paper.summary]\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)         \n",
    "\n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_convert_datetime(date):\n",
    "    return datetime.strptime(date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_other_papers_column(arxiv_paper_df_with_abstract, other_papers):\n",
    "  \n",
    "  df_length = len(arxiv_paper_df_with_abstract) - 1\n",
    "\n",
    "  for other_paper in other_papers:\n",
    "    df_length += 1\n",
    "    arxiv_paper_df_with_abstract.loc[df_length] = other_paper\n",
    "  \n",
    "  arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "  arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)  \n",
    "  \n",
    "  return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperlink(x):\n",
    "    hyperlink= '[Link]' + '(' + x + ')'\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_jouranl_conference(arxiv_paper_df_with_abstract):\n",
    "    \n",
    "    paper_title = arxiv_paper_df_with_abstract['Title']\n",
    "    paper_journal_conference = arxiv_paper_df_with_abstract['Journal/Conference']\n",
    "\n",
    "    for index, (title, journal_conference) in enumerate(zip(paper_title, paper_journal_conference)):\n",
    "\n",
    "        if len(journal_conference) < 2:\n",
    "            clipboard.copy(title)\n",
    "            input_journal_conference = input(\"{} For {}: \".format(\"Input Journal & Conference\", title)) \n",
    "            arxiv_paper_df_with_abstract.loc[index, 'Journal/Conference'] = input_journal_conference\n",
    "\n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_theme(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    paper_title = arxiv_paper_df_with_abstract['Title']\n",
    "    arxiv_paper_df_with_abstract['Theme'] = \"\"\n",
    "    paper_theme = arxiv_paper_df_with_abstract['Theme']\n",
    "\n",
    "    for index, (title, theme) in enumerate(zip(paper_title, paper_theme)):\n",
    "\n",
    "        if len(theme) < 2:\n",
    "            clipboard.copy(title)\n",
    "            input_theme = input(\"{} For {}: \".format(\"Input Theme\", title)) \n",
    "            arxiv_paper_df_with_abstract.loc[index, 'Theme'] = input_theme\n",
    "\n",
    "    arxiv_paper_df_with_abstract = arxiv_paper_df_with_abstract[['Title', 'Journal/Conference', 'Date', 'Author', 'Theme', 'Link', 'Abstract']]\n",
    "\n",
    "    return arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_paper_df(arxiv_paper_df_with_abstract):\n",
    "\n",
    "    arxiv_paper_df_with_abstract = pd.DataFrame(arxiv_paper_df_with_abstract.sort_values(by='Date').reset_index()).drop(['index'], axis='columns')\n",
    "    arxiv_paper_df_with_abstract.index = np.arange(1, len(arxiv_paper_df_with_abstract) + 1)    \n",
    "    arxiv_paper_df = arxiv_paper_df_with_abstract.drop(['Abstract'], axis='columns')\n",
    "\n",
    "    return arxiv_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = [\"1409.0473v7\", \"1409.3215v3\", \"1706.03762v5\", \"1609.08144v2\",\n",
    "             \"1508.07909v5\", \"1301.3781v3\", \"1808.06226v1\", \"1802.05365v2\",\n",
    "             \"1810.04805v2\", \"2104.02395v3\", \"2202.07105v2\", \"1503.02531v1\",\n",
    "             \"1910.01108v4\", \"1908.09355v1\", \"2008.05030v4\", \"1603.08983v6\",\n",
    "             \"1709.01686v1\", \"1804.07461v3\", \"1902.03393v2\", \"2004.02178v2\",\n",
    "             \"2002.10957v2\", \"2012.15828v2\", \"1707.07328v1\", \"1612.00796v2\",\n",
    "             \"1806.00451v1\", \"2106.04570v3\", \"1703.03400v3\", \"1604.00289v3\",\n",
    "             \"2004.09602v1\", \"1603.01025v2\", \"1308.3432v1\", \"1909.05840v2\",\n",
    "             \"1712.05877v1\", \"1811.08886v3\", \"1502.03044v3\", \"1907.05686v5\", \n",
    "             \"2203.06390v1\", \"1609.07061v1\", \"1911.09464v2\", \"2103.13630v3\",\n",
    "             \"1603.05279v4\", \"1906.05714v1\", \"1905.03197v3\", \"1907.10529v3\",\n",
    "             \"1910.13461v1\"]\n",
    "\n",
    "arxiv_paper_df_with_abstract = make_arxiv_paper_df_with_abstract(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_papers = [[\"Model Compression\", \"ACM SIGKDD 2006\", str_convert_datetime(\"2006-08-20\"),\n",
    "                \"Cristian Bucil˘a et al\", \"https://dl.acm.org/doi/abs/10.1145/1150402.1150464\", \n",
    "                \"Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for 'compressing' large, complex ensembles into smaller, faster models, usually without significant loss in performance.\"],\n",
    "                [\"Adaptive Mixtures of Local Experts\", \"MIT Press 1991\", str_convert_datetime(\"1991-03-01\"),\n",
    "                \"Robert A. Jacobs et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.\"],\n",
    "                [\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", \"JMLR 2014\", str_convert_datetime(\"2014-01-01\"),\n",
    "                \"Nitish Srivastava et al\", \"https://ieeexplore.ieee.org/abstract/document/6797059\", \n",
    "                \"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\"],\n",
    "                [\"Linguistic Regularities in Continuous Space Word Representations\", \"NAACL 2013\", str_convert_datetime(\"2013-06-01\"),\n",
    "                \"Tomas Mikolov et al\", \"https://aclanthology.org/N13-1090/\", \n",
    "                \"Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.\"],\n",
    "                 [\"Large-Scale Distributed Language Modeling\", \"IEEE 2007\", str_convert_datetime(\"2007-04-05\"),\n",
    "                \"Ahmad Emami et al\", \"https://ieeexplore.ieee.org/document/4218031\", \n",
    "                \"A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.\"],\n",
    "                 [\"BLEU: a method for automatic evaluation of machine translation\", \"ACL 2002\", str_convert_datetime(\"2002-07-01\"),\n",
    "                \"Kishore Papineni et al\", \"https://dl.acm.org/doi/10.3115/1073083.1073135\", \n",
    "                \"Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.\"],\n",
    "                [\"Large Language Models in Machine Translation\", \"EMNLP 2007\", str_convert_datetime(\"2007-06-01\"),\n",
    "                \"Gloria Brown Wright et al\", \"https://aclanthology.org/D07-1090/\", \n",
    "                \"This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train onup to 2 trillion tokens, resulting in languagemodels having up to 300 billion n-grams. Itis capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbedStupid Backoff, that is inexpensive to trainon large data sets and approaches the qualityof Kneser-Ney Smoothing as the amount oftraining data increases.\"],\n",
    "                [\"Student-Centered Learning in Higher Education\", \" \", str_convert_datetime(\"2011-01-01\"),\n",
    "                \"Thorsten Brants et al\", \"https://files.eric.ed.gov/fulltext/EJ938583.pdf\", \n",
    "                \"In her book, Learner-Centered Teaching, Maryellen Weimer contrasts the practices of teachercentered college teaching and student-centered college teaching in terms of (1) the balance of power in the classroom, (2) the function of the course content, (3) the role of the teacher versus the role of the student, (4) the responsibility of learning, (5) the purpose and processes of evaluation. She then gives some suggestions on how to implement the learner-centered approach. Using Weimer’s five specifications, it has been possible to identify from the pedagogical literature several examples where college teachers are seeking to move toward more student-centered classrooms. This essay reports on innovations used by teachers across the academic and professional spectrum, as well as on their evaluations of their successes.\"],\n",
    "                [\"Optimization as A Model for Few-shot Learning\", \"ICLR 2017\", str_convert_datetime(\"2017-07-22\"),\n",
    "                \"Sachin Ravi et al\", \"https://openreview.net/forum?id=rJY0-Kcll\", \n",
    "                \"Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.\"],\n",
    "                [\"Language Models are Unsupervised Multitask Learners\", \" \", str_convert_datetime(\"2019-06-01\"),\n",
    "                \"Alec Radford et al\", \"https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\", \n",
    "                \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\"]\n",
    "                ]\n",
    "\n",
    "\n",
    "arxiv_paper_df_with_abstract = add_other_papers_column(arxiv_paper_df_with_abstract, other_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract[\"Link\"] = arxiv_paper_df_with_abstract[\"Link\"].apply(hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract = input_jouranl_conference(arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract = input_theme(arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df = make_arxiv_paper_df(arxiv_paper_df_with_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal/Conference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaptive Mixtures of Local Experts</td>\n",
       "      <td>MIT Press 1991</td>\n",
       "      <td>1991-03-01</td>\n",
       "      <td>Robert A. Jacobs et al</td>\n",
       "      <td>Mixture of Experts</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>We present a new supervised learning procedure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLEU: a method for automatic evaluation of mac...</td>\n",
       "      <td>ACL 2002</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>Kishore Papineni et al</td>\n",
       "      <td>Benchmark</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/10.3115/1073083....</td>\n",
       "      <td>Human evaluations of machine translation are e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Compression</td>\n",
       "      <td>ACM SIGKDD 2006</td>\n",
       "      <td>2006-08-20</td>\n",
       "      <td>Cristian Bucil˘a et al</td>\n",
       "      <td>Model Compression</td>\n",
       "      <td>[Link](https://dl.acm.org/doi/abs/10.1145/1150...</td>\n",
       "      <td>Often the best performing supervised learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large-Scale Distributed Language Modeling</td>\n",
       "      <td>IEEE 2007</td>\n",
       "      <td>2007-04-05</td>\n",
       "      <td>Ahmad Emami et al</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/document/42...</td>\n",
       "      <td>A novel distributed language model that has no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Large Language Models in Machine Translation</td>\n",
       "      <td>EMNLP 2007</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>Gloria Brown Wright et al</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>[Link](https://aclanthology.org/D07-1090/)</td>\n",
       "      <td>This paper reports on the benefits of largesca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Student-Centered Learning in Higher Education</td>\n",
       "      <td></td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>Thorsten Brants et al</td>\n",
       "      <td>Explainable Artificial Intelligence (XAI)</td>\n",
       "      <td>[Link](https://files.eric.ed.gov/fulltext/EJ93...</td>\n",
       "      <td>In her book, Learner-Centered Teaching, Maryel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Efficient Estimation of Word Representations i...</td>\n",
       "      <td></td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>Tomas Mikolov et al</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1301.3781v3)</td>\n",
       "      <td>We propose two novel model architectures for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Linguistic Regularities in Continuous Space Wo...</td>\n",
       "      <td>NAACL 2013</td>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>Tomas Mikolov et al</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>[Link](https://aclanthology.org/N13-1090/)</td>\n",
       "      <td>Continuous space language models have recently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Estimating or Propagating Gradients Through St...</td>\n",
       "      <td></td>\n",
       "      <td>2013-08-15</td>\n",
       "      <td>Yoshua Bengio et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1308.3432v1)</td>\n",
       "      <td>Stochastic neurons and hard non-linearities ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dropout: A Simple Way to Prevent Neural Networ...</td>\n",
       "      <td>JMLR 2014</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>Nitish Srivastava et al</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>[Link](https://ieeexplore.ieee.org/abstract/do...</td>\n",
       "      <td>Deep neural nets with a large number of parame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Neural Machine Translation by Jointly Learning...</td>\n",
       "      <td>ICLR  2015</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>Dzmitry Bahdanau et al</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1409.0473v7)</td>\n",
       "      <td>Neural machine translation is a recently propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td>NeurIPS 2014</td>\n",
       "      <td>2014-09-10</td>\n",
       "      <td>Ilya Sutskever et al</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1409.3215v3)</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Show, Attend and Tell: Neural Image Caption Ge...</td>\n",
       "      <td></td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>Kelvin Xu et al</td>\n",
       "      <td>Multimodal Learning</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1502.03044v3)</td>\n",
       "      <td>Inspired by recent work in machine translation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Distilling the Knowledge in a Neural Network</td>\n",
       "      <td>NIPS  2014</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>Geoffrey Hinton et al</td>\n",
       "      <td>Knoweldge Distillation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1503.02531v1)</td>\n",
       "      <td>A very simple way to improve the performance o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Neural Machine Translation of Rare Words with ...</td>\n",
       "      <td>ACL  2016</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Rico Sennrich et al</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1508.07909v5)</td>\n",
       "      <td>Neural machine translation (NMT) models typica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Convolutional Neural Networks using Logarithmi...</td>\n",
       "      <td>ECCV 2016</td>\n",
       "      <td>2016-03-03</td>\n",
       "      <td>Daisuke Miyashita et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1603.01025v2)</td>\n",
       "      <td>Recent advances in convolutional neural networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XNOR-Net: ImageNet Classification Using Binary...</td>\n",
       "      <td>ECCV 2016</td>\n",
       "      <td>2016-03-16</td>\n",
       "      <td>Mohammad Rastegari et al</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1603.05279v4)</td>\n",
       "      <td>We propose two efficient approximations to sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Adaptive Computation Time for Recurrent Neural...</td>\n",
       "      <td></td>\n",
       "      <td>2016-03-29</td>\n",
       "      <td>Alex Graves et al</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1603.08983v6)</td>\n",
       "      <td>This paper introduces Adaptive Computation Tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Building Machines That Learn and Think Like Pe...</td>\n",
       "      <td></td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>Brenden M. Lake et al</td>\n",
       "      <td>Explainable Artificial Intelligence (XAI)</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1604.00289v3)</td>\n",
       "      <td>Recent progress in artificial intelligence (AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Quantized Neural Networks: Training Neural Net...</td>\n",
       "      <td></td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>Itay Hubara et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1609.07061v1)</td>\n",
       "      <td>We introduce a method to train Quantized Neura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Google's Neural Machine Translation System: Br...</td>\n",
       "      <td></td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>Yonghui Wu et al</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1609.08144v2)</td>\n",
       "      <td>Neural Machine Translation (NMT) is an end-to-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Overcoming catastrophic forgetting in neural n...</td>\n",
       "      <td></td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>James Kirkpatrick et al</td>\n",
       "      <td>Continual Learning</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1612.00796v2)</td>\n",
       "      <td>The ability to learn tasks in a sequential fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Model-Agnostic Meta-Learning for Fast Adaptati...</td>\n",
       "      <td>ICML  2017</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>Chelsea Finn et al</td>\n",
       "      <td>Meta Learning</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1703.03400v3)</td>\n",
       "      <td>We propose an algorithm for meta-learning that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>NeurIPS 2017</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>Ashish Vaswani et al</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1706.03762v5)</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Optimization as A Model for Few-shot Learning</td>\n",
       "      <td>ICLR 2017</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>Sachin Ravi et al</td>\n",
       "      <td>Meta Learning</td>\n",
       "      <td>[Link](https://openreview.net/forum?id=rJY0-Kcll)</td>\n",
       "      <td>Though deep neural networks have shown great s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Adversarial Examples for Evaluating Reading Co...</td>\n",
       "      <td>EMNLP  2017</td>\n",
       "      <td>2017-07-23</td>\n",
       "      <td>Robin Jia et al</td>\n",
       "      <td>Natural Language Understanding (NLU)</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1707.07328v1)</td>\n",
       "      <td>Standard accuracy metrics indicate that readin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BranchyNet: Fast Inference via Early Exiting f...</td>\n",
       "      <td></td>\n",
       "      <td>2017-09-06</td>\n",
       "      <td>Surat Teerapittayanon et al</td>\n",
       "      <td>Early Exit</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1709.01686v1)</td>\n",
       "      <td>Deep neural networks are state of the art meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Quantization and Training of Neural Networks f...</td>\n",
       "      <td>CVPR 2018</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>Benoit Jacob et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1712.05877v1)</td>\n",
       "      <td>The rising popularity of intelligent mobile de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Deep contextualized word representations</td>\n",
       "      <td>NAACL  2018</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>Matthew E. Peters et al</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1802.05365v2)</td>\n",
       "      <td>We introduce a new type of deep contextualized...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GLUE: A Multi-Task Benchmark and Analysis Plat...</td>\n",
       "      <td>ICLR  2019</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>Alex Wang et al</td>\n",
       "      <td>Benchmark</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1804.07461v3)</td>\n",
       "      <td>For natural language understanding (NLU) techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Do CIFAR-10 Classifiers Generalize to CIFAR-10?</td>\n",
       "      <td></td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>Benjamin Recht et al</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1806.00451v1)</td>\n",
       "      <td>Machine learning is currently dominated by lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SentencePiece: A simple and language independe...</td>\n",
       "      <td>EMNLP 2018</td>\n",
       "      <td>2018-08-19</td>\n",
       "      <td>Taku Kudo et al</td>\n",
       "      <td>Word Embedding</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1808.06226v1)</td>\n",
       "      <td>This paper describes SentencePiece, a language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td>ACL 2019</td>\n",
       "      <td>2018-10-11</td>\n",
       "      <td>Jacob Devlin et al</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1810.04805v2)</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>HAQ: Hardware-Aware Automated Quantization wit...</td>\n",
       "      <td>CVPR  2019</td>\n",
       "      <td>2018-11-21</td>\n",
       "      <td>Kuan Wang et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1811.08886v3)</td>\n",
       "      <td>Model quantization is a widely used technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Improved Knowledge Distillation via Teacher As...</td>\n",
       "      <td>AAAI  2020</td>\n",
       "      <td>2019-02-09</td>\n",
       "      <td>Seyed-Iman Mirzadeh et al</td>\n",
       "      <td>Knoweldge Distillation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1902.03393v2)</td>\n",
       "      <td>Despite the fact that deep neural networks are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Unified Language Model Pre-training for Natura...</td>\n",
       "      <td>NeurIPS 2019</td>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>Li Dong et al</td>\n",
       "      <td>Pre-Trained Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1905.03197v3)</td>\n",
       "      <td>This paper presents a new Unified pre-trained ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Language Models are Unsupervised Multitask Lea...</td>\n",
       "      <td></td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>Alec Radford et al</td>\n",
       "      <td>Pre-Trained Language Model</td>\n",
       "      <td>[Link](https://www.semanticscholar.org/paper/L...</td>\n",
       "      <td>Natural language processing tasks, such as que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A Multiscale Visualization of Attention in the...</td>\n",
       "      <td>ACL  2019</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>Jesse Vig et al</td>\n",
       "      <td>Explainable Artificial Intelligence (XAI)</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1906.05714v1)</td>\n",
       "      <td>The Transformer is a sequence model that forgo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>And the Bit Goes Down: Revisiting the Quantiza...</td>\n",
       "      <td>ICLR  2020</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>Pierre Stock et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1907.05686v5)</td>\n",
       "      <td>In this paper, we address the problem of reduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SpanBERT: Improving Pre-training by Representi...</td>\n",
       "      <td>TACL 2020</td>\n",
       "      <td>2019-07-24</td>\n",
       "      <td>Mandar Joshi et al</td>\n",
       "      <td>Pre-Trained Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1907.10529v3)</td>\n",
       "      <td>We present SpanBERT, a pre-training method tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Patient Knowledge Distillation for BERT Model ...</td>\n",
       "      <td>EMNLP  2019</td>\n",
       "      <td>2019-08-25</td>\n",
       "      <td>Siqi Sun et al</td>\n",
       "      <td>Knoweldge Distillation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1908.09355v1)</td>\n",
       "      <td>Pre-trained language models such as BERT have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Q-BERT: Hessian Based Ultra Low Precision Quan...</td>\n",
       "      <td>AAAI 2020</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>Sheng Shen et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1909.05840v2)</td>\n",
       "      <td>Transformer based architectures have become de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>NeurIPS 2019</td>\n",
       "      <td>2019-10-02</td>\n",
       "      <td>Victor Sanh et al</td>\n",
       "      <td>Knoweldge Distillation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1910.01108v4)</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>BART: Denoising Sequence-to-Sequence Pre-train...</td>\n",
       "      <td>ACL  2020</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>Mike Lewis et al</td>\n",
       "      <td>Pre-Trained Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1910.13461v1)</td>\n",
       "      <td>We present BART, a denoising autoencoder for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Quantization Networks</td>\n",
       "      <td>CVPR 2019</td>\n",
       "      <td>2019-11-21</td>\n",
       "      <td>Jiwei Yang et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/1911.09464v2)</td>\n",
       "      <td>Although deep neural networks are highly effec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>MiniLM: Deep Self-Attention Distillation for T...</td>\n",
       "      <td>NeurIPS 2020</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>Wenhui Wang et al</td>\n",
       "      <td>Knoweldge Distillation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2002.10957v2)</td>\n",
       "      <td>Pre-trained language models (e.g., BERT (Devli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>FastBERT: a Self-distilling BERT with Adaptive...</td>\n",
       "      <td>ACL  2020</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>Weijie Liu et al</td>\n",
       "      <td>Knoweldge Distillation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2004.02178v2)</td>\n",
       "      <td>Pre-trained language models like BERT have pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Integer Quantization for Deep Learning Inferen...</td>\n",
       "      <td></td>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>Hao Wu et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2004.09602v1)</td>\n",
       "      <td>Quantization techniques can reduce the size of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Reliable Post hoc Explanations: Modeling Uncer...</td>\n",
       "      <td>NeurIPS 2021</td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>Dylan Slack et al</td>\n",
       "      <td>Explainable Artificial Intelligence (XAI)</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2008.05030v4)</td>\n",
       "      <td>As black box explanations are increasingly bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>MiniLMv2: Multi-Head Self-Attention Relation D...</td>\n",
       "      <td>ACL 2021 (Findings)</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>Wenhui Wang et al</td>\n",
       "      <td>Knoweldge Distillation</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2012.15828v2)</td>\n",
       "      <td>We generalize deep self-attention distillation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>A Survey of Quantization Methods for Efficient...</td>\n",
       "      <td></td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>Amir Gholami et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2103.13630v3)</td>\n",
       "      <td>As soon as abstract mathematical computations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Ensemble deep learning: A review</td>\n",
       "      <td>AAAI 2022</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>M. A. Ganaie et al</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2104.02395v3)</td>\n",
       "      <td>Ensemble learning combines several individual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>BERT Learns to Teach: Knowledge Distillation w...</td>\n",
       "      <td>ACL  2022</td>\n",
       "      <td>2021-06-08</td>\n",
       "      <td>Wangchunshu Zhou et al</td>\n",
       "      <td>Meta Learning</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2106.04570v3)</td>\n",
       "      <td>We present Knowledge Distillation with Meta Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>A Survey on Model Compression and Acceleration...</td>\n",
       "      <td>AAAI  2023</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>Canwen Xu et al</td>\n",
       "      <td>Pre-Trained Language Model</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2202.07105v2)</td>\n",
       "      <td>Despite achieving state-of-the-art performance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>BiBERT: Accurate Fully Binarized BERT</td>\n",
       "      <td>ICLR 2022</td>\n",
       "      <td>2022-03-12</td>\n",
       "      <td>Haotong Qin et al</td>\n",
       "      <td>Quantization</td>\n",
       "      <td>[Link](http://arxiv.org/abs/2203.06390v1)</td>\n",
       "      <td>The large pre-trained BERT has achieved remark...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title   Journal/Conference   \n",
       "1                  Adaptive Mixtures of Local Experts       MIT Press 1991  \\\n",
       "2   BLEU: a method for automatic evaluation of mac...             ACL 2002   \n",
       "3                                   Model Compression      ACM SIGKDD 2006   \n",
       "4           Large-Scale Distributed Language Modeling            IEEE 2007   \n",
       "5        Large Language Models in Machine Translation           EMNLP 2007   \n",
       "6       Student-Centered Learning in Higher Education                        \n",
       "7   Efficient Estimation of Word Representations i...                        \n",
       "8   Linguistic Regularities in Continuous Space Wo...           NAACL 2013   \n",
       "9   Estimating or Propagating Gradients Through St...                        \n",
       "10  Dropout: A Simple Way to Prevent Neural Networ...            JMLR 2014   \n",
       "11  Neural Machine Translation by Jointly Learning...           ICLR  2015   \n",
       "12  Sequence to Sequence Learning with Neural Netw...         NeurIPS 2014   \n",
       "13  Show, Attend and Tell: Neural Image Caption Ge...                        \n",
       "14       Distilling the Knowledge in a Neural Network           NIPS  2014   \n",
       "15  Neural Machine Translation of Rare Words with ...            ACL  2016   \n",
       "16  Convolutional Neural Networks using Logarithmi...            ECCV 2016   \n",
       "17  XNOR-Net: ImageNet Classification Using Binary...            ECCV 2016   \n",
       "18  Adaptive Computation Time for Recurrent Neural...                        \n",
       "19  Building Machines That Learn and Think Like Pe...                        \n",
       "20  Quantized Neural Networks: Training Neural Net...                        \n",
       "21  Google's Neural Machine Translation System: Br...                        \n",
       "22  Overcoming catastrophic forgetting in neural n...                        \n",
       "23  Model-Agnostic Meta-Learning for Fast Adaptati...           ICML  2017   \n",
       "24                          Attention Is All You Need         NeurIPS 2017   \n",
       "25      Optimization as A Model for Few-shot Learning            ICLR 2017   \n",
       "26  Adversarial Examples for Evaluating Reading Co...          EMNLP  2017   \n",
       "27  BranchyNet: Fast Inference via Early Exiting f...                        \n",
       "28  Quantization and Training of Neural Networks f...            CVPR 2018   \n",
       "29           Deep contextualized word representations          NAACL  2018   \n",
       "30  GLUE: A Multi-Task Benchmark and Analysis Plat...           ICLR  2019   \n",
       "31    Do CIFAR-10 Classifiers Generalize to CIFAR-10?                        \n",
       "32  SentencePiece: A simple and language independe...           EMNLP 2018   \n",
       "33  BERT: Pre-training of Deep Bidirectional Trans...             ACL 2019   \n",
       "34  HAQ: Hardware-Aware Automated Quantization wit...           CVPR  2019   \n",
       "35  Improved Knowledge Distillation via Teacher As...           AAAI  2020   \n",
       "36  Unified Language Model Pre-training for Natura...         NeurIPS 2019   \n",
       "37  Language Models are Unsupervised Multitask Lea...                        \n",
       "38  A Multiscale Visualization of Attention in the...            ACL  2019   \n",
       "39  And the Bit Goes Down: Revisiting the Quantiza...           ICLR  2020   \n",
       "40  SpanBERT: Improving Pre-training by Representi...            TACL 2020   \n",
       "41  Patient Knowledge Distillation for BERT Model ...          EMNLP  2019   \n",
       "42  Q-BERT: Hessian Based Ultra Low Precision Quan...            AAAI 2020   \n",
       "43  DistilBERT, a distilled version of BERT: small...         NeurIPS 2019   \n",
       "44  BART: Denoising Sequence-to-Sequence Pre-train...            ACL  2020   \n",
       "45                              Quantization Networks            CVPR 2019   \n",
       "46  MiniLM: Deep Self-Attention Distillation for T...         NeurIPS 2020   \n",
       "47  FastBERT: a Self-distilling BERT with Adaptive...            ACL  2020   \n",
       "48  Integer Quantization for Deep Learning Inferen...                        \n",
       "49  Reliable Post hoc Explanations: Modeling Uncer...         NeurIPS 2021   \n",
       "50  MiniLMv2: Multi-Head Self-Attention Relation D...  ACL 2021 (Findings)   \n",
       "51  A Survey of Quantization Methods for Efficient...                        \n",
       "52                   Ensemble deep learning: A review            AAAI 2022   \n",
       "53  BERT Learns to Teach: Knowledge Distillation w...            ACL  2022   \n",
       "54  A Survey on Model Compression and Acceleration...           AAAI  2023   \n",
       "55              BiBERT: Accurate Fully Binarized BERT            ICLR 2022   \n",
       "\n",
       "          Date                       Author   \n",
       "1   1991-03-01       Robert A. Jacobs et al  \\\n",
       "2   2002-07-01       Kishore Papineni et al   \n",
       "3   2006-08-20       Cristian Bucil˘a et al   \n",
       "4   2007-04-05            Ahmad Emami et al   \n",
       "5   2007-06-01    Gloria Brown Wright et al   \n",
       "6   2011-01-01        Thorsten Brants et al   \n",
       "7   2013-01-16          Tomas Mikolov et al   \n",
       "8   2013-06-01          Tomas Mikolov et al   \n",
       "9   2013-08-15          Yoshua Bengio et al   \n",
       "10  2014-01-01      Nitish Srivastava et al   \n",
       "11  2014-09-01       Dzmitry Bahdanau et al   \n",
       "12  2014-09-10         Ilya Sutskever et al   \n",
       "13  2015-02-10              Kelvin Xu et al   \n",
       "14  2015-03-09        Geoffrey Hinton et al   \n",
       "15  2015-08-31          Rico Sennrich et al   \n",
       "16  2016-03-03      Daisuke Miyashita et al   \n",
       "17  2016-03-16     Mohammad Rastegari et al   \n",
       "18  2016-03-29            Alex Graves et al   \n",
       "19  2016-04-01        Brenden M. Lake et al   \n",
       "20  2016-09-22            Itay Hubara et al   \n",
       "21  2016-09-26             Yonghui Wu et al   \n",
       "22  2016-12-02      James Kirkpatrick et al   \n",
       "23  2017-03-09           Chelsea Finn et al   \n",
       "24  2017-06-12         Ashish Vaswani et al   \n",
       "25  2017-07-22            Sachin Ravi et al   \n",
       "26  2017-07-23              Robin Jia et al   \n",
       "27  2017-09-06  Surat Teerapittayanon et al   \n",
       "28  2017-12-15           Benoit Jacob et al   \n",
       "29  2018-02-15      Matthew E. Peters et al   \n",
       "30  2018-04-20              Alex Wang et al   \n",
       "31  2018-06-01         Benjamin Recht et al   \n",
       "32  2018-08-19              Taku Kudo et al   \n",
       "33  2018-10-11           Jacob Devlin et al   \n",
       "34  2018-11-21              Kuan Wang et al   \n",
       "35  2019-02-09    Seyed-Iman Mirzadeh et al   \n",
       "36  2019-05-08                Li Dong et al   \n",
       "37  2019-06-01           Alec Radford et al   \n",
       "38  2019-06-12              Jesse Vig et al   \n",
       "39  2019-07-12           Pierre Stock et al   \n",
       "40  2019-07-24           Mandar Joshi et al   \n",
       "41  2019-08-25               Siqi Sun et al   \n",
       "42  2019-09-12             Sheng Shen et al   \n",
       "43  2019-10-02            Victor Sanh et al   \n",
       "44  2019-10-29             Mike Lewis et al   \n",
       "45  2019-11-21             Jiwei Yang et al   \n",
       "46  2020-02-25            Wenhui Wang et al   \n",
       "47  2020-04-05             Weijie Liu et al   \n",
       "48  2020-04-20                 Hao Wu et al   \n",
       "49  2020-08-11            Dylan Slack et al   \n",
       "50  2020-12-31            Wenhui Wang et al   \n",
       "51  2021-03-25           Amir Gholami et al   \n",
       "52  2021-04-06           M. A. Ganaie et al   \n",
       "53  2021-06-08       Wangchunshu Zhou et al   \n",
       "54  2022-02-15              Canwen Xu et al   \n",
       "55  2022-03-12            Haotong Qin et al   \n",
       "\n",
       "                                        Theme   \n",
       "1                          Mixture of Experts  \\\n",
       "2                                   Benchmark   \n",
       "3                           Model Compression   \n",
       "4                              Word Embedding   \n",
       "5                         Machine Translation   \n",
       "6   Explainable Artificial Intelligence (XAI)   \n",
       "7                              Word Embedding   \n",
       "8                              Word Embedding   \n",
       "9                                Quantization   \n",
       "10                             Neural Network   \n",
       "11                        Machine Translation   \n",
       "12                        Machine Translation   \n",
       "13                        Multimodal Learning   \n",
       "14                     Knoweldge Distillation   \n",
       "15                        Machine Translation   \n",
       "16                               Quantization   \n",
       "17                       Image Classification   \n",
       "18                             Neural Network   \n",
       "19  Explainable Artificial Intelligence (XAI)   \n",
       "20                               Quantization   \n",
       "21                        Machine Translation   \n",
       "22                         Continual Learning   \n",
       "23                              Meta Learning   \n",
       "24                        Machine Translation   \n",
       "25                              Meta Learning   \n",
       "26       Natural Language Understanding (NLU)   \n",
       "27                                 Early Exit   \n",
       "28                               Quantization   \n",
       "29                             Word Embedding   \n",
       "30                                  Benchmark   \n",
       "31                       Image Classification   \n",
       "32                             Word Embedding   \n",
       "33                             Neural Network   \n",
       "34                               Quantization   \n",
       "35                     Knoweldge Distillation   \n",
       "36                 Pre-Trained Language Model   \n",
       "37                 Pre-Trained Language Model   \n",
       "38  Explainable Artificial Intelligence (XAI)   \n",
       "39                               Quantization   \n",
       "40                 Pre-Trained Language Model   \n",
       "41                     Knoweldge Distillation   \n",
       "42                               Quantization   \n",
       "43                     Knoweldge Distillation   \n",
       "44                 Pre-Trained Language Model   \n",
       "45                              Quantization    \n",
       "46                     Knoweldge Distillation   \n",
       "47                     Knoweldge Distillation   \n",
       "48                               Quantization   \n",
       "49  Explainable Artificial Intelligence (XAI)   \n",
       "50                     Knoweldge Distillation   \n",
       "51                               Quantization   \n",
       "52                                   Ensemble   \n",
       "53                              Meta Learning   \n",
       "54                 Pre-Trained Language Model   \n",
       "55                               Quantization   \n",
       "\n",
       "                                                 Link   \n",
       "1   [Link](https://ieeexplore.ieee.org/abstract/do...  \\\n",
       "2   [Link](https://dl.acm.org/doi/10.3115/1073083....   \n",
       "3   [Link](https://dl.acm.org/doi/abs/10.1145/1150...   \n",
       "4   [Link](https://ieeexplore.ieee.org/document/42...   \n",
       "5          [Link](https://aclanthology.org/D07-1090/)   \n",
       "6   [Link](https://files.eric.ed.gov/fulltext/EJ93...   \n",
       "7            [Link](http://arxiv.org/abs/1301.3781v3)   \n",
       "8          [Link](https://aclanthology.org/N13-1090/)   \n",
       "9            [Link](http://arxiv.org/abs/1308.3432v1)   \n",
       "10  [Link](https://ieeexplore.ieee.org/abstract/do...   \n",
       "11           [Link](http://arxiv.org/abs/1409.0473v7)   \n",
       "12           [Link](http://arxiv.org/abs/1409.3215v3)   \n",
       "13          [Link](http://arxiv.org/abs/1502.03044v3)   \n",
       "14          [Link](http://arxiv.org/abs/1503.02531v1)   \n",
       "15          [Link](http://arxiv.org/abs/1508.07909v5)   \n",
       "16          [Link](http://arxiv.org/abs/1603.01025v2)   \n",
       "17          [Link](http://arxiv.org/abs/1603.05279v4)   \n",
       "18          [Link](http://arxiv.org/abs/1603.08983v6)   \n",
       "19          [Link](http://arxiv.org/abs/1604.00289v3)   \n",
       "20          [Link](http://arxiv.org/abs/1609.07061v1)   \n",
       "21          [Link](http://arxiv.org/abs/1609.08144v2)   \n",
       "22          [Link](http://arxiv.org/abs/1612.00796v2)   \n",
       "23          [Link](http://arxiv.org/abs/1703.03400v3)   \n",
       "24          [Link](http://arxiv.org/abs/1706.03762v5)   \n",
       "25  [Link](https://openreview.net/forum?id=rJY0-Kcll)   \n",
       "26          [Link](http://arxiv.org/abs/1707.07328v1)   \n",
       "27          [Link](http://arxiv.org/abs/1709.01686v1)   \n",
       "28          [Link](http://arxiv.org/abs/1712.05877v1)   \n",
       "29          [Link](http://arxiv.org/abs/1802.05365v2)   \n",
       "30          [Link](http://arxiv.org/abs/1804.07461v3)   \n",
       "31          [Link](http://arxiv.org/abs/1806.00451v1)   \n",
       "32          [Link](http://arxiv.org/abs/1808.06226v1)   \n",
       "33          [Link](http://arxiv.org/abs/1810.04805v2)   \n",
       "34          [Link](http://arxiv.org/abs/1811.08886v3)   \n",
       "35          [Link](http://arxiv.org/abs/1902.03393v2)   \n",
       "36          [Link](http://arxiv.org/abs/1905.03197v3)   \n",
       "37  [Link](https://www.semanticscholar.org/paper/L...   \n",
       "38          [Link](http://arxiv.org/abs/1906.05714v1)   \n",
       "39          [Link](http://arxiv.org/abs/1907.05686v5)   \n",
       "40          [Link](http://arxiv.org/abs/1907.10529v3)   \n",
       "41          [Link](http://arxiv.org/abs/1908.09355v1)   \n",
       "42          [Link](http://arxiv.org/abs/1909.05840v2)   \n",
       "43          [Link](http://arxiv.org/abs/1910.01108v4)   \n",
       "44          [Link](http://arxiv.org/abs/1910.13461v1)   \n",
       "45          [Link](http://arxiv.org/abs/1911.09464v2)   \n",
       "46          [Link](http://arxiv.org/abs/2002.10957v2)   \n",
       "47          [Link](http://arxiv.org/abs/2004.02178v2)   \n",
       "48          [Link](http://arxiv.org/abs/2004.09602v1)   \n",
       "49          [Link](http://arxiv.org/abs/2008.05030v4)   \n",
       "50          [Link](http://arxiv.org/abs/2012.15828v2)   \n",
       "51          [Link](http://arxiv.org/abs/2103.13630v3)   \n",
       "52          [Link](http://arxiv.org/abs/2104.02395v3)   \n",
       "53          [Link](http://arxiv.org/abs/2106.04570v3)   \n",
       "54          [Link](http://arxiv.org/abs/2202.07105v2)   \n",
       "55          [Link](http://arxiv.org/abs/2203.06390v1)   \n",
       "\n",
       "                                             Abstract  \n",
       "1   We present a new supervised learning procedure...  \n",
       "2   Human evaluations of machine translation are e...  \n",
       "3   Often the best performing supervised learning ...  \n",
       "4   A novel distributed language model that has no...  \n",
       "5   This paper reports on the benefits of largesca...  \n",
       "6   In her book, Learner-Centered Teaching, Maryel...  \n",
       "7   We propose two novel model architectures for c...  \n",
       "8   Continuous space language models have recently...  \n",
       "9   Stochastic neurons and hard non-linearities ca...  \n",
       "10  Deep neural nets with a large number of parame...  \n",
       "11  Neural machine translation is a recently propo...  \n",
       "12  Deep Neural Networks (DNNs) are powerful model...  \n",
       "13  Inspired by recent work in machine translation...  \n",
       "14  A very simple way to improve the performance o...  \n",
       "15  Neural machine translation (NMT) models typica...  \n",
       "16  Recent advances in convolutional neural networ...  \n",
       "17  We propose two efficient approximations to sta...  \n",
       "18  This paper introduces Adaptive Computation Tim...  \n",
       "19  Recent progress in artificial intelligence (AI...  \n",
       "20  We introduce a method to train Quantized Neura...  \n",
       "21  Neural Machine Translation (NMT) is an end-to-...  \n",
       "22  The ability to learn tasks in a sequential fas...  \n",
       "23  We propose an algorithm for meta-learning that...  \n",
       "24  The dominant sequence transduction models are ...  \n",
       "25  Though deep neural networks have shown great s...  \n",
       "26  Standard accuracy metrics indicate that readin...  \n",
       "27  Deep neural networks are state of the art meth...  \n",
       "28  The rising popularity of intelligent mobile de...  \n",
       "29  We introduce a new type of deep contextualized...  \n",
       "30  For natural language understanding (NLU) techn...  \n",
       "31  Machine learning is currently dominated by lar...  \n",
       "32  This paper describes SentencePiece, a language...  \n",
       "33  We introduce a new language representation mod...  \n",
       "34  Model quantization is a widely used technique ...  \n",
       "35  Despite the fact that deep neural networks are...  \n",
       "36  This paper presents a new Unified pre-trained ...  \n",
       "37  Natural language processing tasks, such as que...  \n",
       "38  The Transformer is a sequence model that forgo...  \n",
       "39  In this paper, we address the problem of reduc...  \n",
       "40  We present SpanBERT, a pre-training method tha...  \n",
       "41  Pre-trained language models such as BERT have ...  \n",
       "42  Transformer based architectures have become de...  \n",
       "43  As Transfer Learning from large-scale pre-trai...  \n",
       "44  We present BART, a denoising autoencoder for p...  \n",
       "45  Although deep neural networks are highly effec...  \n",
       "46  Pre-trained language models (e.g., BERT (Devli...  \n",
       "47  Pre-trained language models like BERT have pro...  \n",
       "48  Quantization techniques can reduce the size of...  \n",
       "49  As black box explanations are increasingly bei...  \n",
       "50  We generalize deep self-attention distillation...  \n",
       "51  As soon as abstract mathematical computations ...  \n",
       "52  Ensemble learning combines several individual ...  \n",
       "53  We present Knowledge Distillation with Meta Le...  \n",
       "54  Despite achieving state-of-the-art performance...  \n",
       "55  The large pre-trained BERT has achieved remark...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper_df_with_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_df_with_abstract.to_excel(\"arxiv_paper_df_with_abstract.xlsx\")\n",
    "arxiv_paper_df.to_excel(\"arxiv_paper_df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataframe on Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Excel to Markdown Converter](https://tabletomarkdown.com/convert-spreadsheet-to-markdown/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
